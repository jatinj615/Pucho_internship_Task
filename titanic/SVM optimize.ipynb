{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Survived','Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Pclass','Sex', 'Age', 'SibSp', 'Parch','Fare', 'Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_1 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1] = le_1.fit_transform(X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_2 = LabelEncoder()\n",
    "X[:, 6] = le_2.fit_transform(X[:, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=X, columns=['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, X_train, X_test, y_train, y_test, train = True):\n",
    "#     print accuracy score, classification report, confusion metrics\n",
    "    if train:\n",
    "#         training performance\n",
    "        print('Train Result : \\n')\n",
    "        print('Accuracy Score {0:.4f}\\n'.format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "        print('Classification Report : \\n {} \\n'.format(classification_report(y_train, clf.predict(X_train))))\n",
    "        print('Confusion Metrics : \\n {} \\n'.format(confusion_matrix(y_train, clf.predict(X_train))))\n",
    "        \n",
    "        res = cross_val_score(clf, X_train, y_train, cv = 10, scoring='accuracy')\n",
    "        print('Average Accuracy : {0:.4f}\\n'.format(np.mean(res)))\n",
    "        print('Accuracy SD : {0:.4f}\\n'.format(np.std(res)))\n",
    "        \n",
    "    elif train == False:\n",
    "#         test performance\n",
    "        print('Test Result : \\n')\n",
    "        print('Accuracy Score {0:.4f}\\n'.format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "        print('Classification Report : \\n {}\\n'.format(classification_report(y_test, clf.predict(X_test))))\n",
    "        print('Confusion Metrics : \\n {} \\n'.format(confusion_matrix(y_test, clf.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C = 5, degree=2, cache_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=5, cache_size=100, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=2, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result : \n",
      "\n",
      "Accuracy Score 0.8418\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.92      0.88       344\n",
      "          1       0.85      0.72      0.78       225\n",
      "\n",
      "avg / total       0.84      0.84      0.84       569\n",
      " \n",
      "\n",
      "Confusion Metrics : \n",
      " [[316  28]\n",
      " [ 62 163]] \n",
      "\n",
      "Average Accuracy : 0.8138\n",
      "\n",
      "Accuracy SD : 0.0531\n",
      "\n",
      "Test Result : \n",
      "\n",
      "Accuracy Score 0.8112\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.89      0.84        80\n",
      "          1       0.83      0.71      0.77        63\n",
      "\n",
      "avg / total       0.81      0.81      0.81       143\n",
      "\n",
      "\n",
      "Confusion Metrics : \n",
      " [[71  9]\n",
      " [18 45]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(clf, X_train, X_test, y_train, y_test, train=True)\n",
    "print_score(clf, X_train, X_test, y_train, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C': [0.001, 0.01, 0.1, 1, 2, 3],\n",
    "          'cache_size': [100, 200, 400, 500],\n",
    "          'kernel': ['rbf', 'linear', 'poly'],\n",
    "          'degree': range(1, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv = GridSearchCV(clf, param_grid=params, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 2, 3], 'cache_size': [100, 200, 400, 500], 'kernel': ['rbf', 'linear', 'poly'], 'degree': range(1, 5)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result : \n",
      "\n",
      "Accuracy Score 0.8418\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.94      0.88       344\n",
      "          1       0.88      0.69      0.78       225\n",
      "\n",
      "avg / total       0.85      0.84      0.84       569\n",
      " \n",
      "\n",
      "Confusion Metrics : \n",
      " [[323  21]\n",
      " [ 69 156]] \n",
      "\n",
      "Average Accuracy : 0.7996\n",
      "\n",
      "Accuracy SD : 0.0438\n",
      "\n",
      "Test Result : \n",
      "\n",
      "Accuracy Score 0.8252\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.91      0.85        80\n",
      "          1       0.87      0.71      0.78        63\n",
      "\n",
      "avg / total       0.83      0.83      0.82       143\n",
      "\n",
      "\n",
      "Confusion Metrics : \n",
      " [[73  7]\n",
      " [18 45]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(grid_cv, X_train, X_test, y_train, y_test, train=True)\n",
    "print_score(grid_cv, X_train, X_test, y_train, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=100, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=1, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
