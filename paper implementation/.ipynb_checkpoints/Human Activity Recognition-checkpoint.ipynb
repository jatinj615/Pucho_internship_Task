{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('X_train.txt', header=None)\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
       "1  0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
       "2  0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
       "3  0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
       "4  0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
       "\n",
       "        7         8         9      ...          551       552       553  \\\n",
       "0 -0.983185 -0.923527 -0.934724    ...    -0.074323 -0.298676 -0.710304   \n",
       "1 -0.974914 -0.957686 -0.943068    ...     0.158075 -0.595051 -0.861499   \n",
       "2 -0.963668 -0.977469 -0.938692    ...     0.414503 -0.390748 -0.760104   \n",
       "3 -0.982750 -0.989302 -0.938692    ...     0.404573 -0.117290 -0.482845   \n",
       "4 -0.979672 -0.990441 -0.942469    ...     0.087753 -0.351471 -0.699205   \n",
       "\n",
       "        554       555       556       557       558       559       560  \n",
       "0 -0.112754  0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627  \n",
       "1  0.053477 -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317  \n",
       "2 -0.118559  0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118  \n",
       "3 -0.036788 -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663  \n",
       "4  0.123320  0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892  \n",
       "\n",
       "[5 rows x 561 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274488</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.109141</td>\n",
       "      <td>-0.605438</td>\n",
       "      <td>-0.510938</td>\n",
       "      <td>-0.604754</td>\n",
       "      <td>-0.630512</td>\n",
       "      <td>-0.526907</td>\n",
       "      <td>-0.606150</td>\n",
       "      <td>-0.468604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125293</td>\n",
       "      <td>-0.307009</td>\n",
       "      <td>-0.625294</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.489547</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>-0.056515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.448734</td>\n",
       "      <td>0.502645</td>\n",
       "      <td>0.418687</td>\n",
       "      <td>0.424073</td>\n",
       "      <td>0.485942</td>\n",
       "      <td>0.414122</td>\n",
       "      <td>0.544547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.321011</td>\n",
       "      <td>0.307584</td>\n",
       "      <td>0.336787</td>\n",
       "      <td>0.448306</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.477975</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.297480</td>\n",
       "      <td>0.279122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995357</td>\n",
       "      <td>-0.999765</td>\n",
       "      <td>-0.976580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262975</td>\n",
       "      <td>-0.024863</td>\n",
       "      <td>-0.120993</td>\n",
       "      <td>-0.992754</td>\n",
       "      <td>-0.978129</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>-0.993591</td>\n",
       "      <td>-0.978162</td>\n",
       "      <td>-0.980251</td>\n",
       "      <td>-0.936219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023692</td>\n",
       "      <td>-0.542602</td>\n",
       "      <td>-0.845573</td>\n",
       "      <td>-0.121527</td>\n",
       "      <td>-0.289549</td>\n",
       "      <td>-0.482273</td>\n",
       "      <td>-0.376341</td>\n",
       "      <td>-0.812065</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.143414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277193</td>\n",
       "      <td>-0.017219</td>\n",
       "      <td>-0.108676</td>\n",
       "      <td>-0.946196</td>\n",
       "      <td>-0.851897</td>\n",
       "      <td>-0.859365</td>\n",
       "      <td>-0.950709</td>\n",
       "      <td>-0.857328</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.881637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>-0.343685</td>\n",
       "      <td>-0.711692</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.709417</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288461</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.097794</td>\n",
       "      <td>-0.242813</td>\n",
       "      <td>-0.034231</td>\n",
       "      <td>-0.262415</td>\n",
       "      <td>-0.292680</td>\n",
       "      <td>-0.066701</td>\n",
       "      <td>-0.265671</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289096</td>\n",
       "      <td>-0.126979</td>\n",
       "      <td>-0.503878</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.292861</td>\n",
       "      <td>0.506187</td>\n",
       "      <td>0.359368</td>\n",
       "      <td>-0.509079</td>\n",
       "      <td>0.248353</td>\n",
       "      <td>0.107659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.274488    -0.017695    -0.109141    -0.605438    -0.510938   \n",
       "std       0.070261     0.040811     0.056635     0.448734     0.502645   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -0.999873   \n",
       "25%       0.262975    -0.024863    -0.120993    -0.992754    -0.978129   \n",
       "50%       0.277193    -0.017219    -0.108676    -0.946196    -0.851897   \n",
       "75%       0.288461    -0.010783    -0.097794    -0.242813    -0.034231   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.916238   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean     -0.604754    -0.630512    -0.526907    -0.606150    -0.468604   \n",
       "std       0.418687     0.424073     0.485942     0.414122     0.544547   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.980233    -0.993591    -0.978162    -0.980251    -0.936219   \n",
       "50%      -0.859365    -0.950709    -0.857328    -0.857143    -0.881637   \n",
       "75%      -0.262415    -0.292680    -0.066701    -0.265671    -0.017129   \n",
       "max       1.000000     1.000000     0.967664     1.000000     1.000000   \n",
       "\n",
       "          ...               551          552          553          554  \\\n",
       "count     ...       7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      ...          0.125293    -0.307009    -0.625294     0.008684   \n",
       "std       ...          0.250994     0.321011     0.307584     0.336787   \n",
       "min       ...         -1.000000    -0.995357    -0.999765    -0.976580   \n",
       "25%       ...         -0.023692    -0.542602    -0.845573    -0.121527   \n",
       "50%       ...          0.134000    -0.343685    -0.711692     0.009509   \n",
       "75%       ...          0.289096    -0.126979    -0.503878     0.150865   \n",
       "max       ...          0.946700     0.989538     0.956845     1.000000   \n",
       "\n",
       "               555          556          557          558          559  \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.002186     0.008726    -0.005981    -0.489547     0.058593   \n",
       "std       0.448306     0.608303     0.477975     0.511807     0.297480   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.289549    -0.482273    -0.376341    -0.812065    -0.017885   \n",
       "50%       0.008943     0.008735    -0.000368    -0.709417     0.182071   \n",
       "75%       0.292861     0.506187     0.359368    -0.509079     0.248353   \n",
       "max       1.000000     0.998702     0.996078     1.000000     0.478157   \n",
       "\n",
       "               560  \n",
       "count  7352.000000  \n",
       "mean     -0.056515  \n",
       "std       0.279122  \n",
       "min      -1.000000  \n",
       "25%      -0.143414  \n",
       "50%       0.003181  \n",
       "75%       0.107659  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  5\n",
       "1  5\n",
       "2  5\n",
       "3  5\n",
       "4  5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_fwf('y_train.txt',header=None)\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5  6\n",
       "0  0  0  0  0  1  0\n",
       "1  0  0  0  0  1  0\n",
       "2  0  0  0  0  1  0\n",
       "3  0  0  0  0  1  0\n",
       "4  0  0  0  0  1  0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_fwf('X_test.txt',header=None)\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.read_fwf('y_test.txt', header=None)\n",
    "y_test = pd.get_dummies(test_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=60, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=60, units=35)`\n",
      "  \n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=35)`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"softmax\", units=6)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clf = Sequential()\n",
    "clf.add(Dense(output_dim = 35, kernel_initializer='uniform', activation='relu', input_dim = 60))\n",
    "clf.add(Dropout(rate=0.5))\n",
    "clf.add(Dense(output_dim = 35, kernel_initializer='uniform', activation='relu'))\n",
    "clf.add(Dropout(rate= 0.6))\n",
    "clf.add(Dense(output_dim = 6, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 1s 175us/step - loss: 1.1800 - acc: 0.4562 - val_loss: 0.6912 - val_acc: 0.6943\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.5845 - acc: 0.7357 - val_loss: 0.3896 - val_acc: 0.8694\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.3869 - acc: 0.8490 - val_loss: 0.2888 - val_acc: 0.8941\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.3029 - acc: 0.8862 - val_loss: 0.2559 - val_acc: 0.9057\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.2570 - acc: 0.9053 - val_loss: 0.2334 - val_acc: 0.9175\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.2438 - acc: 0.9097 - val_loss: 0.2164 - val_acc: 0.9257\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 1s 146us/step - loss: 0.2120 - acc: 0.9221 - val_loss: 0.2165 - val_acc: 0.9233\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1925 - acc: 0.9354 - val_loss: 0.2113 - val_acc: 0.9223\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 1s 146us/step - loss: 0.1880 - acc: 0.9324 - val_loss: 0.2127 - val_acc: 0.9253\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1774 - acc: 0.9359 - val_loss: 0.2044 - val_acc: 0.9281\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 1s 162us/step - loss: 0.1714 - acc: 0.9421 - val_loss: 0.2190 - val_acc: 0.9257\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 1s 161us/step - loss: 0.1557 - acc: 0.9465 - val_loss: 0.2018 - val_acc: 0.9311\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1550 - acc: 0.9468 - val_loss: 0.2064 - val_acc: 0.9321\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 1s 163us/step - loss: 0.1530 - acc: 0.9467 - val_loss: 0.2012 - val_acc: 0.9345\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1516 - acc: 0.9491 - val_loss: 0.2206 - val_acc: 0.9277\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 1s 146us/step - loss: 0.1409 - acc: 0.9525 - val_loss: 0.2107 - val_acc: 0.9338\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.1473 - acc: 0.9490 - val_loss: 0.2220 - val_acc: 0.9264\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 1s 160us/step - loss: 0.1351 - acc: 0.9540 - val_loss: 0.2087 - val_acc: 0.9301\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1274 - acc: 0.9593 - val_loss: 0.2083 - val_acc: 0.9342\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 1s 162us/step - loss: 0.1275 - acc: 0.9547 - val_loss: 0.2139 - val_acc: 0.9311\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.1268 - acc: 0.9555 - val_loss: 0.2198 - val_acc: 0.9318\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1214 - acc: 0.9592 - val_loss: 0.2358 - val_acc: 0.9274\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.1243 - acc: 0.9546 - val_loss: 0.2146 - val_acc: 0.9352\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.1195 - acc: 0.9597 - val_loss: 0.2235 - val_acc: 0.9342\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1164 - acc: 0.9591 - val_loss: 0.2272 - val_acc: 0.9332\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1196 - acc: 0.9562 - val_loss: 0.2135 - val_acc: 0.9348\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.1106 - acc: 0.9612 - val_loss: 0.2208 - val_acc: 0.9345\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 1s 170us/step - loss: 0.1168 - acc: 0.9601 - val_loss: 0.2397 - val_acc: 0.9274\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.1013 - acc: 0.9614 - val_loss: 0.2245 - val_acc: 0.9355\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.1077 - acc: 0.9626 - val_loss: 0.2322 - val_acc: 0.9345\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 1s 165us/step - loss: 0.1041 - acc: 0.9637 - val_loss: 0.2397 - val_acc: 0.9362\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.1188 - acc: 0.9576 - val_loss: 0.2892 - val_acc: 0.9267\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.1080 - acc: 0.9640 - val_loss: 0.2431 - val_acc: 0.9338\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.1066 - acc: 0.9630 - val_loss: 0.2574 - val_acc: 0.9311\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 1s 178us/step - loss: 0.1019 - acc: 0.9652 - val_loss: 0.2743 - val_acc: 0.9267\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1018 - acc: 0.9650 - val_loss: 0.2624 - val_acc: 0.9294\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 1s 171us/step - loss: 0.0982 - acc: 0.9656 - val_loss: 0.2802 - val_acc: 0.9277\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1076 - acc: 0.9642 - val_loss: 0.2614 - val_acc: 0.9301\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.0984 - acc: 0.9656 - val_loss: 0.2831 - val_acc: 0.9277\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.0953 - acc: 0.9667 - val_loss: 0.2788 - val_acc: 0.9291\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.1006 - acc: 0.9640 - val_loss: 0.2893 - val_acc: 0.9281\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.0967 - acc: 0.9663 - val_loss: 0.2840 - val_acc: 0.9294\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.0945 - acc: 0.9664 - val_loss: 0.2637 - val_acc: 0.9348\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.1010 - acc: 0.9663 - val_loss: 0.2941 - val_acc: 0.9298\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0959 - acc: 0.9675 - val_loss: 0.2953 - val_acc: 0.9301\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.0975 - acc: 0.9682 - val_loss: 0.3031 - val_acc: 0.9287\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0968 - acc: 0.9661 - val_loss: 0.2745 - val_acc: 0.9321\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.1004 - acc: 0.9652 - val_loss: 0.2954 - val_acc: 0.9281\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.0928 - acc: 0.9679 - val_loss: 0.2978 - val_acc: 0.9308\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0927 - acc: 0.9684 - val_loss: 0.3047 - val_acc: 0.9274\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 1s 158us/step - loss: 0.0927 - acc: 0.9682 - val_loss: 0.2800 - val_acc: 0.9328\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 1s 173us/step - loss: 0.0852 - acc: 0.9716 - val_loss: 0.2927 - val_acc: 0.9298\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0841 - acc: 0.9712 - val_loss: 0.3127 - val_acc: 0.9274\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 1s 158us/step - loss: 0.0873 - acc: 0.9714 - val_loss: 0.3071 - val_acc: 0.9264\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 1s 162us/step - loss: 0.0945 - acc: 0.9684 - val_loss: 0.3177 - val_acc: 0.9291\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.0842 - acc: 0.9703 - val_loss: 0.3249 - val_acc: 0.9291\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.0876 - acc: 0.9714 - val_loss: 0.3093 - val_acc: 0.9281\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.0874 - acc: 0.9725 - val_loss: 0.3121 - val_acc: 0.9328\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.0817 - acc: 0.9701 - val_loss: 0.3191 - val_acc: 0.9328\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352/7352 [==============================] - 1s 175us/step - loss: 0.0815 - acc: 0.9691 - val_loss: 0.2834 - val_acc: 0.9389\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 1s 172us/step - loss: 0.0899 - acc: 0.9706 - val_loss: 0.2914 - val_acc: 0.9328\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 1s 118us/step - loss: 0.0942 - acc: 0.9669 - val_loss: 0.2998 - val_acc: 0.9304\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 2s 208us/step - loss: 0.0836 - acc: 0.9705 - val_loss: 0.3322 - val_acc: 0.9270\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 2s 218us/step - loss: 0.0896 - acc: 0.9701 - val_loss: 0.3321 - val_acc: 0.9318\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 1s 181us/step - loss: 0.0912 - acc: 0.9687 - val_loss: 0.3020 - val_acc: 0.9321\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 1s 194us/step - loss: 0.0916 - acc: 0.9702 - val_loss: 0.3329 - val_acc: 0.9274\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 1s 181us/step - loss: 0.0880 - acc: 0.9708 - val_loss: 0.3282 - val_acc: 0.9287\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 1s 187us/step - loss: 0.0831 - acc: 0.9713 - val_loss: 0.3532 - val_acc: 0.9291\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 1s 202us/step - loss: 0.0882 - acc: 0.9712 - val_loss: 0.3290 - val_acc: 0.9321\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 1s 170us/step - loss: 0.0860 - acc: 0.9712 - val_loss: 0.3271 - val_acc: 0.9332\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.0828 - acc: 0.9720 - val_loss: 0.3660 - val_acc: 0.9264\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.0896 - acc: 0.9671 - val_loss: 0.3248 - val_acc: 0.9315\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.0827 - acc: 0.9718 - val_loss: 0.3680 - val_acc: 0.9311\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.0846 - acc: 0.9693 - val_loss: 0.3205 - val_acc: 0.9338\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.0847 - acc: 0.9682 - val_loss: 0.3732 - val_acc: 0.9264\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0815 - acc: 0.9703 - val_loss: 0.3552 - val_acc: 0.9301\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 1s 146us/step - loss: 0.0819 - acc: 0.9712 - val_loss: 0.3644 - val_acc: 0.9287\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 1s 146us/step - loss: 0.0842 - acc: 0.9698 - val_loss: 0.3693 - val_acc: 0.9298\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.0815 - acc: 0.9724 - val_loss: 0.3537 - val_acc: 0.9318\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.0798 - acc: 0.9740 - val_loss: 0.3552 - val_acc: 0.9304\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0888 - acc: 0.9701 - val_loss: 0.3333 - val_acc: 0.9291\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.0822 - acc: 0.9717 - val_loss: 0.3257 - val_acc: 0.9311\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.0808 - acc: 0.9702 - val_loss: 0.3506 - val_acc: 0.9311\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0826 - acc: 0.9710 - val_loss: 0.3561 - val_acc: 0.9311\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.0834 - acc: 0.9703 - val_loss: 0.3493 - val_acc: 0.9281\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 1s 150us/step - loss: 0.0765 - acc: 0.9744 - val_loss: 0.3938 - val_acc: 0.9253\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.0794 - acc: 0.9717 - val_loss: 0.3580 - val_acc: 0.9287\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.0752 - acc: 0.9744 - val_loss: 0.3618 - val_acc: 0.9264\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.0810 - acc: 0.9721 - val_loss: 0.3419 - val_acc: 0.9304\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 1s 163us/step - loss: 0.0805 - acc: 0.9714 - val_loss: 0.3431 - val_acc: 0.9311\n",
      "Epoch 91/100\n",
      "7352/7352 [==============================] - 1s 161us/step - loss: 0.0810 - acc: 0.9706 - val_loss: 0.3584 - val_acc: 0.9318\n",
      "Epoch 92/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.0808 - acc: 0.9718 - val_loss: 0.3886 - val_acc: 0.9264\n",
      "Epoch 93/100\n",
      "7352/7352 [==============================] - 1s 160us/step - loss: 0.0789 - acc: 0.9712 - val_loss: 0.3777 - val_acc: 0.9287\n",
      "Epoch 94/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.0758 - acc: 0.9740 - val_loss: 0.3747 - val_acc: 0.9294\n",
      "Epoch 95/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.0831 - acc: 0.9701 - val_loss: 0.3824 - val_acc: 0.9274\n",
      "Epoch 96/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0843 - acc: 0.9740 - val_loss: 0.3506 - val_acc: 0.9291\n",
      "Epoch 97/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.0840 - acc: 0.9705 - val_loss: 0.4088 - val_acc: 0.9264\n",
      "Epoch 98/100\n",
      "7352/7352 [==============================] - 1s 198us/step - loss: 0.0761 - acc: 0.9747 - val_loss: 0.4372 - val_acc: 0.9230\n",
      "Epoch 99/100\n",
      "7352/7352 [==============================] - 1s 161us/step - loss: 0.0868 - acc: 0.9695 - val_loss: 0.3770 - val_acc: 0.9267\n",
      "Epoch 100/100\n",
      "7352/7352 [==============================] - 1s 158us/step - loss: 0.0855 - acc: 0.9675 - val_loss: 0.3591 - val_acc: 0.9321\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(X_pca, y, batch_size=32, epochs = 100, validation_data=(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Sequential()\n",
    "clf.add(LSTM(units=120, activation='relu', input_shape=(None, 60), dropout=0.3, recurrent_dropout=0.3))\n",
    "clf.add(Dense(35, activation='relu'))\n",
    "clf.add(Dropout(rate=0.4))\n",
    "clf.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 3s 373us/step - loss: 0.9513 - acc: 0.6696 - val_loss: 0.2409 - val_acc: 0.9209\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 2s 282us/step - loss: 0.3496 - acc: 0.8829 - val_loss: 0.1920 - val_acc: 0.9226\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 2s 294us/step - loss: 0.2613 - acc: 0.9101 - val_loss: 0.1756 - val_acc: 0.9325\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.2253 - acc: 0.9222 - val_loss: 0.1755 - val_acc: 0.9338\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.2091 - acc: 0.9289 - val_loss: 0.1741 - val_acc: 0.9372\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.1938 - acc: 0.9313 - val_loss: 0.1760 - val_acc: 0.9352\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.1744 - acc: 0.9369 - val_loss: 0.1534 - val_acc: 0.9454\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 2s 274us/step - loss: 0.1688 - acc: 0.9403 - val_loss: 0.1713 - val_acc: 0.9362\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.1636 - acc: 0.9437 - val_loss: 0.1966 - val_acc: 0.9338\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 2s 264us/step - loss: 0.1547 - acc: 0.9455 - val_loss: 0.1714 - val_acc: 0.9396\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1448 - acc: 0.9467 - val_loss: 0.1853 - val_acc: 0.9355\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.1420 - acc: 0.9468 - val_loss: 0.1737 - val_acc: 0.9362\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 2s 259us/step - loss: 0.1334 - acc: 0.9514 - val_loss: 0.1876 - val_acc: 0.9396\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1244 - acc: 0.9544 - val_loss: 0.1768 - val_acc: 0.9406\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 2s 272us/step - loss: 0.1269 - acc: 0.9569 - val_loss: 0.1854 - val_acc: 0.9379\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.1187 - acc: 0.9580 - val_loss: 0.1627 - val_acc: 0.9444\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.1206 - acc: 0.9565 - val_loss: 0.1857 - val_acc: 0.9413\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1179 - acc: 0.9563 - val_loss: 0.1652 - val_acc: 0.9450\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 2s 263us/step - loss: 0.1169 - acc: 0.9582 - val_loss: 0.2002 - val_acc: 0.9325\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 2s 271us/step - loss: 0.1136 - acc: 0.9573 - val_loss: 0.2166 - val_acc: 0.9315\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.1019 - acc: 0.9615 - val_loss: 0.1952 - val_acc: 0.9372\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.0991 - acc: 0.9637 - val_loss: 0.2039 - val_acc: 0.9365\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 2s 274us/step - loss: 0.0932 - acc: 0.9638 - val_loss: 0.1696 - val_acc: 0.9474\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.0990 - acc: 0.9626 - val_loss: 0.2233 - val_acc: 0.9325\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 2s 272us/step - loss: 0.0910 - acc: 0.9669 - val_loss: 0.1784 - val_acc: 0.9444\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 2s 259us/step - loss: 0.0912 - acc: 0.9682 - val_loss: 0.1832 - val_acc: 0.9413\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.0963 - acc: 0.9648 - val_loss: 0.1948 - val_acc: 0.9396\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.0868 - acc: 0.9663 - val_loss: 0.2220 - val_acc: 0.9352\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 2s 271us/step - loss: 0.0916 - acc: 0.9671 - val_loss: 0.2377 - val_acc: 0.9315\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 2s 329us/step - loss: 0.0934 - acc: 0.9657 - val_loss: 0.1887 - val_acc: 0.9376\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 2s 273us/step - loss: 0.0744 - acc: 0.9732 - val_loss: 0.2108 - val_acc: 0.9386\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 2s 277us/step - loss: 0.0834 - acc: 0.9701 - val_loss: 0.1775 - val_acc: 0.9423\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 2s 321us/step - loss: 0.0751 - acc: 0.9717 - val_loss: 0.1987 - val_acc: 0.9393\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 2s 272us/step - loss: 0.0845 - acc: 0.9690 - val_loss: 0.1644 - val_acc: 0.9457\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.0802 - acc: 0.9717 - val_loss: 0.1808 - val_acc: 0.9457\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 2s 307us/step - loss: 0.0762 - acc: 0.9717 - val_loss: 0.1871 - val_acc: 0.9454\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 2s 277us/step - loss: 0.0697 - acc: 0.9743 - val_loss: 0.1749 - val_acc: 0.9454\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 2s 293us/step - loss: 0.0716 - acc: 0.9740 - val_loss: 0.1902 - val_acc: 0.9416\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 2s 282us/step - loss: 0.0707 - acc: 0.9742 - val_loss: 0.1889 - val_acc: 0.9447\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 2s 274us/step - loss: 0.0766 - acc: 0.9752 - val_loss: 0.1887 - val_acc: 0.9444\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 2s 318us/step - loss: 0.0765 - acc: 0.9725 - val_loss: 0.1890 - val_acc: 0.9450\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 2s 307us/step - loss: 0.0710 - acc: 0.9729 - val_loss: 0.1597 - val_acc: 0.9511\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 3s 347us/step - loss: 0.0647 - acc: 0.9755 - val_loss: 0.2129 - val_acc: 0.9406\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 2s 331us/step - loss: 0.0703 - acc: 0.9742 - val_loss: 0.1870 - val_acc: 0.9471\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 2s 302us/step - loss: 0.0615 - acc: 0.9785 - val_loss: 0.2176 - val_acc: 0.9413\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.0697 - acc: 0.9748 - val_loss: 0.2012 - val_acc: 0.9420\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 2s 225us/step - loss: 0.0705 - acc: 0.9743 - val_loss: 0.2020 - val_acc: 0.9447\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 2s 217us/step - loss: 0.0693 - acc: 0.9750 - val_loss: 0.2104 - val_acc: 0.9399\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 2s 227us/step - loss: 0.0698 - acc: 0.9757 - val_loss: 0.2312 - val_acc: 0.9403\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 2s 221us/step - loss: 0.0639 - acc: 0.9774 - val_loss: 0.2120 - val_acc: 0.9413\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 2s 212us/step - loss: 0.0667 - acc: 0.9776 - val_loss: 0.2014 - val_acc: 0.9430\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 2s 217us/step - loss: 0.0612 - acc: 0.9766 - val_loss: 0.1875 - val_acc: 0.9440\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 2s 212us/step - loss: 0.0601 - acc: 0.9761 - val_loss: 0.1930 - val_acc: 0.9450\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 2s 216us/step - loss: 0.0566 - acc: 0.9803 - val_loss: 0.2473 - val_acc: 0.9376\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 2s 212us/step - loss: 0.0594 - acc: 0.9786 - val_loss: 0.1933 - val_acc: 0.9457\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 2s 217us/step - loss: 0.0587 - acc: 0.9808 - val_loss: 0.2052 - val_acc: 0.9437\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 2s 241us/step - loss: 0.0573 - acc: 0.9799 - val_loss: 0.1946 - val_acc: 0.9447\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 2s 216us/step - loss: 0.0621 - acc: 0.9774 - val_loss: 0.1788 - val_acc: 0.9477\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 2s 222us/step - loss: 0.0547 - acc: 0.9795 - val_loss: 0.1847 - val_acc: 0.9488\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352/7352 [==============================] - 2s 224us/step - loss: 0.0592 - acc: 0.9784 - val_loss: 0.1924 - val_acc: 0.9447\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 2s 213us/step - loss: 0.0543 - acc: 0.9793 - val_loss: 0.2010 - val_acc: 0.9437\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 2s 214us/step - loss: 0.0543 - acc: 0.9812 - val_loss: 0.2058 - val_acc: 0.9444\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 2s 209us/step - loss: 0.0550 - acc: 0.9796 - val_loss: 0.1982 - val_acc: 0.9433\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 2s 212us/step - loss: 0.0548 - acc: 0.9814 - val_loss: 0.2120 - val_acc: 0.9454\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 2s 211us/step - loss: 0.0549 - acc: 0.9804 - val_loss: 0.2246 - val_acc: 0.9440\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 2s 209us/step - loss: 0.0570 - acc: 0.9791 - val_loss: 0.1867 - val_acc: 0.9447\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 2s 215us/step - loss: 0.0534 - acc: 0.9811 - val_loss: 0.2225 - val_acc: 0.9399\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 2s 211us/step - loss: 0.0509 - acc: 0.9819 - val_loss: 0.1906 - val_acc: 0.9474\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 2s 209us/step - loss: 0.0513 - acc: 0.9812 - val_loss: 0.2029 - val_acc: 0.9474\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 2s 211us/step - loss: 0.0510 - acc: 0.9827 - val_loss: 0.1844 - val_acc: 0.9501\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 2s 222us/step - loss: 0.0542 - acc: 0.9796 - val_loss: 0.2445 - val_acc: 0.9382\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 2s 288us/step - loss: 0.0522 - acc: 0.9818 - val_loss: 0.2013 - val_acc: 0.9430\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 2s 289us/step - loss: 0.0507 - acc: 0.9839 - val_loss: 0.1963 - val_acc: 0.9491\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.0523 - acc: 0.9818 - val_loss: 0.2018 - val_acc: 0.9484\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.0517 - acc: 0.9818 - val_loss: 0.1948 - val_acc: 0.9484\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.0464 - acc: 0.9834 - val_loss: 0.1993 - val_acc: 0.9460\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.0396 - acc: 0.9848 - val_loss: 0.1896 - val_acc: 0.9508\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 2s 279us/step - loss: 0.0535 - acc: 0.9807 - val_loss: 0.1902 - val_acc: 0.9511\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 2s 288us/step - loss: 0.0437 - acc: 0.9830 - val_loss: 0.1897 - val_acc: 0.9471\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 2s 283us/step - loss: 0.0519 - acc: 0.9816 - val_loss: 0.1974 - val_acc: 0.9440\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 2s 264us/step - loss: 0.0450 - acc: 0.9849 - val_loss: 0.2023 - val_acc: 0.9437\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 2s 267us/step - loss: 0.0529 - acc: 0.9803 - val_loss: 0.2154 - val_acc: 0.9420\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 2s 271us/step - loss: 0.0534 - acc: 0.9819 - val_loss: 0.2061 - val_acc: 0.9396\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 2s 267us/step - loss: 0.0448 - acc: 0.9859 - val_loss: 0.1971 - val_acc: 0.9471\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 2s 264us/step - loss: 0.0438 - acc: 0.9849 - val_loss: 0.2256 - val_acc: 0.9423\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.0504 - acc: 0.9820 - val_loss: 0.1828 - val_acc: 0.9525\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 2s 264us/step - loss: 0.0504 - acc: 0.9827 - val_loss: 0.1859 - val_acc: 0.9491\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 2s 267us/step - loss: 0.0415 - acc: 0.9848 - val_loss: 0.1922 - val_acc: 0.9488\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 2s 270us/step - loss: 0.0448 - acc: 0.9854 - val_loss: 0.2082 - val_acc: 0.9477\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.0480 - acc: 0.9811 - val_loss: 0.1733 - val_acc: 0.9518\n",
      "Epoch 91/100\n",
      "7352/7352 [==============================] - 2s 270us/step - loss: 0.0420 - acc: 0.9852 - val_loss: 0.1609 - val_acc: 0.9528\n",
      "Epoch 92/100\n",
      "7352/7352 [==============================] - 2s 267us/step - loss: 0.0463 - acc: 0.9837 - val_loss: 0.2428 - val_acc: 0.9423\n",
      "Epoch 93/100\n",
      "7352/7352 [==============================] - 2s 272us/step - loss: 0.0366 - acc: 0.9868 - val_loss: 0.2170 - val_acc: 0.9457\n",
      "Epoch 94/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.0405 - acc: 0.9850 - val_loss: 0.2217 - val_acc: 0.9474\n",
      "Epoch 95/100\n",
      "7352/7352 [==============================] - 2s 273us/step - loss: 0.0489 - acc: 0.9833 - val_loss: 0.2273 - val_acc: 0.9413\n",
      "Epoch 96/100\n",
      "7352/7352 [==============================] - 2s 268us/step - loss: 0.0437 - acc: 0.9830 - val_loss: 0.2153 - val_acc: 0.9457\n",
      "Epoch 97/100\n",
      "7352/7352 [==============================] - 2s 273us/step - loss: 0.0422 - acc: 0.9844 - val_loss: 0.2088 - val_acc: 0.9454\n",
      "Epoch 98/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.0419 - acc: 0.9861 - val_loss: 0.2536 - val_acc: 0.9420\n",
      "Epoch 99/100\n",
      "7352/7352 [==============================] - 2s 283us/step - loss: 0.0385 - acc: 0.9883 - val_loss: 0.2416 - val_acc: 0.9460\n",
      "Epoch 100/100\n",
      "7352/7352 [==============================] - 2s 287us/step - loss: 0.0439 - acc: 0.9844 - val_loss: 0.2442 - val_acc: 0.9413\n"
     ]
    }
   ],
   "source": [
    "X_pca = np.reshape(X_pca, (X_pca.shape[0], 1, X_pca.shape[1]))\n",
    "X_test_pca = np.reshape(X_test_pca, (X_test_pca.shape[0], 1, X_test_pca.shape[1]))\n",
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(X_pca, y, batch_size=32, epochs = 100, validation_data=(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
