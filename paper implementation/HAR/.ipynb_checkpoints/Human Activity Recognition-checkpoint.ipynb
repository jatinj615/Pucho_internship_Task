{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('X_train.txt', header=None)\n",
    "X_train = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
       "1  0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
       "2  0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
       "3  0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
       "4  0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
       "\n",
       "        7         8         9      ...          551       552       553  \\\n",
       "0 -0.983185 -0.923527 -0.934724    ...    -0.074323 -0.298676 -0.710304   \n",
       "1 -0.974914 -0.957686 -0.943068    ...     0.158075 -0.595051 -0.861499   \n",
       "2 -0.963668 -0.977469 -0.938692    ...     0.414503 -0.390748 -0.760104   \n",
       "3 -0.982750 -0.989302 -0.938692    ...     0.404573 -0.117290 -0.482845   \n",
       "4 -0.979672 -0.990441 -0.942469    ...     0.087753 -0.351471 -0.699205   \n",
       "\n",
       "        554       555       556       557       558       559       560  \n",
       "0 -0.112754  0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627  \n",
       "1  0.053477 -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317  \n",
       "2 -0.118559  0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118  \n",
       "3 -0.036788 -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663  \n",
       "4  0.123320  0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892  \n",
       "\n",
       "[5 rows x 561 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274488</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.109141</td>\n",
       "      <td>-0.605438</td>\n",
       "      <td>-0.510938</td>\n",
       "      <td>-0.604754</td>\n",
       "      <td>-0.630512</td>\n",
       "      <td>-0.526907</td>\n",
       "      <td>-0.606150</td>\n",
       "      <td>-0.468604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125293</td>\n",
       "      <td>-0.307009</td>\n",
       "      <td>-0.625294</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.489547</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>-0.056515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.448734</td>\n",
       "      <td>0.502645</td>\n",
       "      <td>0.418687</td>\n",
       "      <td>0.424073</td>\n",
       "      <td>0.485942</td>\n",
       "      <td>0.414122</td>\n",
       "      <td>0.544547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.321011</td>\n",
       "      <td>0.307584</td>\n",
       "      <td>0.336787</td>\n",
       "      <td>0.448306</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.477975</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.297480</td>\n",
       "      <td>0.279122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995357</td>\n",
       "      <td>-0.999765</td>\n",
       "      <td>-0.976580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262975</td>\n",
       "      <td>-0.024863</td>\n",
       "      <td>-0.120993</td>\n",
       "      <td>-0.992754</td>\n",
       "      <td>-0.978129</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>-0.993591</td>\n",
       "      <td>-0.978162</td>\n",
       "      <td>-0.980251</td>\n",
       "      <td>-0.936219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023692</td>\n",
       "      <td>-0.542602</td>\n",
       "      <td>-0.845573</td>\n",
       "      <td>-0.121527</td>\n",
       "      <td>-0.289549</td>\n",
       "      <td>-0.482273</td>\n",
       "      <td>-0.376341</td>\n",
       "      <td>-0.812065</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.143414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277193</td>\n",
       "      <td>-0.017219</td>\n",
       "      <td>-0.108676</td>\n",
       "      <td>-0.946196</td>\n",
       "      <td>-0.851897</td>\n",
       "      <td>-0.859365</td>\n",
       "      <td>-0.950709</td>\n",
       "      <td>-0.857328</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.881637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>-0.343685</td>\n",
       "      <td>-0.711692</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.709417</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288461</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.097794</td>\n",
       "      <td>-0.242813</td>\n",
       "      <td>-0.034231</td>\n",
       "      <td>-0.262415</td>\n",
       "      <td>-0.292680</td>\n",
       "      <td>-0.066701</td>\n",
       "      <td>-0.265671</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289096</td>\n",
       "      <td>-0.126979</td>\n",
       "      <td>-0.503878</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.292861</td>\n",
       "      <td>0.506187</td>\n",
       "      <td>0.359368</td>\n",
       "      <td>-0.509079</td>\n",
       "      <td>0.248353</td>\n",
       "      <td>0.107659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.274488    -0.017695    -0.109141    -0.605438    -0.510938   \n",
       "std       0.070261     0.040811     0.056635     0.448734     0.502645   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -0.999873   \n",
       "25%       0.262975    -0.024863    -0.120993    -0.992754    -0.978129   \n",
       "50%       0.277193    -0.017219    -0.108676    -0.946196    -0.851897   \n",
       "75%       0.288461    -0.010783    -0.097794    -0.242813    -0.034231   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.916238   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean     -0.604754    -0.630512    -0.526907    -0.606150    -0.468604   \n",
       "std       0.418687     0.424073     0.485942     0.414122     0.544547   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.980233    -0.993591    -0.978162    -0.980251    -0.936219   \n",
       "50%      -0.859365    -0.950709    -0.857328    -0.857143    -0.881637   \n",
       "75%      -0.262415    -0.292680    -0.066701    -0.265671    -0.017129   \n",
       "max       1.000000     1.000000     0.967664     1.000000     1.000000   \n",
       "\n",
       "          ...               551          552          553          554  \\\n",
       "count     ...       7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      ...          0.125293    -0.307009    -0.625294     0.008684   \n",
       "std       ...          0.250994     0.321011     0.307584     0.336787   \n",
       "min       ...         -1.000000    -0.995357    -0.999765    -0.976580   \n",
       "25%       ...         -0.023692    -0.542602    -0.845573    -0.121527   \n",
       "50%       ...          0.134000    -0.343685    -0.711692     0.009509   \n",
       "75%       ...          0.289096    -0.126979    -0.503878     0.150865   \n",
       "max       ...          0.946700     0.989538     0.956845     1.000000   \n",
       "\n",
       "               555          556          557          558          559  \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.002186     0.008726    -0.005981    -0.489547     0.058593   \n",
       "std       0.448306     0.608303     0.477975     0.511807     0.297480   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.289549    -0.482273    -0.376341    -0.812065    -0.017885   \n",
       "50%       0.008943     0.008735    -0.000368    -0.709417     0.182071   \n",
       "75%       0.292861     0.506187     0.359368    -0.509079     0.248353   \n",
       "max       1.000000     0.998702     0.996078     1.000000     0.478157   \n",
       "\n",
       "               560  \n",
       "count  7352.000000  \n",
       "mean     -0.056515  \n",
       "std       0.279122  \n",
       "min      -1.000000  \n",
       "25%      -0.143414  \n",
       "50%       0.003181  \n",
       "75%       0.107659  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  5\n",
       "1  5\n",
       "2  5\n",
       "3  5\n",
       "4  5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_fwf('y_train.txt',header=None)\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5  6\n",
       "0  0  0  0  0  1  0\n",
       "1  0  0  0  0  1  0\n",
       "2  0  0  0  0  1  0\n",
       "3  0  0  0  0  1  0\n",
       "4  0  0  0  0  1  0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 6)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_fwf('X_test.txt',header=None)\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.read_fwf('y_test.txt', header=None)\n",
    "y_test = pd.get_dummies(test_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=60, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=60, units=35)`\n",
      "  \n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=35)`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"softmax\", units=6)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clf = Sequential()\n",
    "clf.add(Dense(output_dim = 35, kernel_initializer='uniform', activation='relu', input_dim = 60))\n",
    "clf.add(Dropout(rate=0.6))\n",
    "clf.add(Dense(output_dim = 35, kernel_initializer='uniform', activation='relu'))\n",
    "clf.add(Dropout(rate= 0.6))\n",
    "clf.add(Dense(output_dim = 6, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 2s 284us/step - loss: 1.2423 - acc: 0.4053 - val_loss: 0.7823 - val_acc: 0.5786\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 1s 136us/step - loss: 0.7891 - acc: 0.5824 - val_loss: 0.6716 - val_acc: 0.6664\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.6801 - acc: 0.6825 - val_loss: 0.5574 - val_acc: 0.7937\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 1s 133us/step - loss: 0.5877 - acc: 0.7556 - val_loss: 0.4525 - val_acc: 0.8863\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.5050 - acc: 0.8079 - val_loss: 0.3202 - val_acc: 0.9287\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.4105 - acc: 0.8641 - val_loss: 0.2431 - val_acc: 0.9362\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.3402 - acc: 0.8911 - val_loss: 0.2153 - val_acc: 0.9277\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 1s 136us/step - loss: 0.3151 - acc: 0.8996 - val_loss: 0.2032 - val_acc: 0.9369\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.2907 - acc: 0.9030 - val_loss: 0.1946 - val_acc: 0.9345\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.2816 - acc: 0.9059 - val_loss: 0.1943 - val_acc: 0.9365\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 1s 142us/step - loss: 0.2720 - acc: 0.9089 - val_loss: 0.1957 - val_acc: 0.9386\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.2528 - acc: 0.9153 - val_loss: 0.1969 - val_acc: 0.9321\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.2425 - acc: 0.9203 - val_loss: 0.1929 - val_acc: 0.9318\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.2374 - acc: 0.9234 - val_loss: 0.1912 - val_acc: 0.9257\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 1s 136us/step - loss: 0.2453 - acc: 0.9188 - val_loss: 0.2080 - val_acc: 0.9321\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.2278 - acc: 0.9253 - val_loss: 0.1959 - val_acc: 0.9284\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.2378 - acc: 0.9189 - val_loss: 0.2218 - val_acc: 0.9203\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.2247 - acc: 0.9274 - val_loss: 0.2127 - val_acc: 0.9325\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.2225 - acc: 0.9257 - val_loss: 0.1965 - val_acc: 0.9369\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.2245 - acc: 0.9271 - val_loss: 0.2017 - val_acc: 0.9308\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.2178 - acc: 0.9297 - val_loss: 0.2040 - val_acc: 0.9338\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.2074 - acc: 0.9313 - val_loss: 0.2208 - val_acc: 0.9281\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.2022 - acc: 0.9306 - val_loss: 0.1979 - val_acc: 0.9345\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.2113 - acc: 0.9320 - val_loss: 0.2353 - val_acc: 0.9247\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.2105 - acc: 0.9302 - val_loss: 0.2353 - val_acc: 0.9308\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.2109 - acc: 0.9309 - val_loss: 0.2225 - val_acc: 0.9260\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.2008 - acc: 0.9389 - val_loss: 0.2101 - val_acc: 0.9355\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1946 - acc: 0.9381 - val_loss: 0.2307 - val_acc: 0.9247\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.2028 - acc: 0.9324 - val_loss: 0.2236 - val_acc: 0.9325\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.1897 - acc: 0.9369 - val_loss: 0.2447 - val_acc: 0.9328\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 1s 136us/step - loss: 0.2008 - acc: 0.9340 - val_loss: 0.2402 - val_acc: 0.9338\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.1919 - acc: 0.9372 - val_loss: 0.2456 - val_acc: 0.9284\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.1942 - acc: 0.9348 - val_loss: 0.2273 - val_acc: 0.9369\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.1921 - acc: 0.9353 - val_loss: 0.2469 - val_acc: 0.9287\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1933 - acc: 0.9366 - val_loss: 0.2605 - val_acc: 0.9216\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.1857 - acc: 0.9378 - val_loss: 0.2370 - val_acc: 0.9332\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.1839 - acc: 0.9374 - val_loss: 0.2498 - val_acc: 0.9301\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1915 - acc: 0.9331 - val_loss: 0.2384 - val_acc: 0.9304\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.1898 - acc: 0.9366 - val_loss: 0.2243 - val_acc: 0.9328\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.1920 - acc: 0.9355 - val_loss: 0.2732 - val_acc: 0.9277\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.1701 - acc: 0.9418 - val_loss: 0.2403 - val_acc: 0.9332\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1719 - acc: 0.9425 - val_loss: 0.2372 - val_acc: 0.9362\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.1703 - acc: 0.9414 - val_loss: 0.2550 - val_acc: 0.9321\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.1863 - acc: 0.9403 - val_loss: 0.2718 - val_acc: 0.9335\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.1884 - acc: 0.9387 - val_loss: 0.2624 - val_acc: 0.9321\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1792 - acc: 0.9416 - val_loss: 0.2925 - val_acc: 0.9270\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.1671 - acc: 0.9444 - val_loss: 0.2984 - val_acc: 0.9240\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.1730 - acc: 0.9427 - val_loss: 0.3015 - val_acc: 0.9247\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1833 - acc: 0.9425 - val_loss: 0.3019 - val_acc: 0.9274\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.1695 - acc: 0.9436 - val_loss: 0.2966 - val_acc: 0.9260\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.1662 - acc: 0.9440 - val_loss: 0.3241 - val_acc: 0.9233\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.1655 - acc: 0.9457 - val_loss: 0.2835 - val_acc: 0.9301\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.1832 - acc: 0.9399 - val_loss: 0.2630 - val_acc: 0.9311\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1815 - acc: 0.9389 - val_loss: 0.2868 - val_acc: 0.9301\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.1717 - acc: 0.9425 - val_loss: 0.3280 - val_acc: 0.9264\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.1733 - acc: 0.9414 - val_loss: 0.2839 - val_acc: 0.9291\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.1600 - acc: 0.9452 - val_loss: 0.2976 - val_acc: 0.9328\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.1786 - acc: 0.9459 - val_loss: 0.3199 - val_acc: 0.9287\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1640 - acc: 0.9464 - val_loss: 0.2933 - val_acc: 0.9359\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.1638 - acc: 0.9452 - val_loss: 0.3071 - val_acc: 0.9287\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.1670 - acc: 0.9463 - val_loss: 0.3261 - val_acc: 0.9298\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 1s 204us/step - loss: 0.1687 - acc: 0.9455 - val_loss: 0.3000 - val_acc: 0.9253\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 1s 175us/step - loss: 0.1721 - acc: 0.9455 - val_loss: 0.2988 - val_acc: 0.9301\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 1s 171us/step - loss: 0.1717 - acc: 0.9437 - val_loss: 0.3532 - val_acc: 0.9281\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 1s 160us/step - loss: 0.1709 - acc: 0.9449 - val_loss: 0.3182 - val_acc: 0.9287\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.1629 - acc: 0.9490 - val_loss: 0.3016 - val_acc: 0.9308\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 1s 160us/step - loss: 0.1633 - acc: 0.9446 - val_loss: 0.3247 - val_acc: 0.9270\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1622 - acc: 0.9483 - val_loss: 0.3262 - val_acc: 0.9301\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.1681 - acc: 0.9437 - val_loss: 0.3149 - val_acc: 0.9284\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.1736 - acc: 0.9463 - val_loss: 0.3283 - val_acc: 0.9291\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1637 - acc: 0.9464 - val_loss: 0.3397 - val_acc: 0.9281\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.1658 - acc: 0.9468 - val_loss: 0.3559 - val_acc: 0.9277\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1710 - acc: 0.9449 - val_loss: 0.3304 - val_acc: 0.9287\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.1657 - acc: 0.9427 - val_loss: 0.3402 - val_acc: 0.9284\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1654 - acc: 0.9459 - val_loss: 0.3606 - val_acc: 0.9277\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 1s 150us/step - loss: 0.1654 - acc: 0.9453 - val_loss: 0.3146 - val_acc: 0.9304\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.1697 - acc: 0.9440 - val_loss: 0.3223 - val_acc: 0.9301\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1569 - acc: 0.9472 - val_loss: 0.3449 - val_acc: 0.9281\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1662 - acc: 0.9461 - val_loss: 0.3670 - val_acc: 0.9257\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.1495 - acc: 0.9512 - val_loss: 0.3753 - val_acc: 0.9281\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.1689 - acc: 0.9453 - val_loss: 0.3699 - val_acc: 0.9253\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1655 - acc: 0.9422 - val_loss: 0.3465 - val_acc: 0.9291\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.1624 - acc: 0.9468 - val_loss: 0.3473 - val_acc: 0.9274\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 1s 161us/step - loss: 0.1675 - acc: 0.9449 - val_loss: 0.3715 - val_acc: 0.9264\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1644 - acc: 0.9471 - val_loss: 0.3782 - val_acc: 0.9237\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.1697 - acc: 0.9448 - val_loss: 0.3560 - val_acc: 0.9253\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1657 - acc: 0.9459 - val_loss: 0.3606 - val_acc: 0.9240\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1638 - acc: 0.9460 - val_loss: 0.3621 - val_acc: 0.9209\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1634 - acc: 0.9464 - val_loss: 0.3468 - val_acc: 0.9257\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1651 - acc: 0.9482 - val_loss: 0.3768 - val_acc: 0.9240\n",
      "Epoch 91/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1645 - acc: 0.9463 - val_loss: 0.3872 - val_acc: 0.9226\n",
      "Epoch 92/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.1511 - acc: 0.9497 - val_loss: 0.3782 - val_acc: 0.9243\n",
      "Epoch 93/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1555 - acc: 0.9493 - val_loss: 0.4025 - val_acc: 0.9237\n",
      "Epoch 94/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1495 - acc: 0.9536 - val_loss: 0.4206 - val_acc: 0.9199\n",
      "Epoch 95/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.1483 - acc: 0.9491 - val_loss: 0.3908 - val_acc: 0.9277\n",
      "Epoch 96/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.1746 - acc: 0.9407 - val_loss: 0.3932 - val_acc: 0.9253\n",
      "Epoch 97/100\n",
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.1704 - acc: 0.9441 - val_loss: 0.4070 - val_acc: 0.9287\n",
      "Epoch 98/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1568 - acc: 0.9497 - val_loss: 0.4075 - val_acc: 0.9277\n",
      "Epoch 99/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.1639 - acc: 0.9493 - val_loss: 0.4041 - val_acc: 0.9250\n",
      "Epoch 100/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.1570 - acc: 0.9499 - val_loss: 0.3746 - val_acc: 0.9230\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(X_train_pca, y_train, batch_size=32, epochs = 100, validation_data=(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Sequential()\n",
    "clf.add(LSTM(units=120, activation='relu', input_shape=(None, 60), dropout=0.4, recurrent_dropout=0.4))\n",
    "clf.add(Dense(35, activation='relu'))\n",
    "clf.add(Dropout(rate=0.5))\n",
    "clf.add(Dense(35, activation='relu'))\n",
    "clf.add(Dropout(rate=0.5))\n",
    "clf.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 3s 353us/step - loss: 1.0764 - acc: 0.6070 - val_loss: 0.3137 - val_acc: 0.8945\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 2s 241us/step - loss: 0.4481 - acc: 0.8485 - val_loss: 0.1944 - val_acc: 0.9298\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 2s 282us/step - loss: 0.3720 - acc: 0.8713 - val_loss: 0.1811 - val_acc: 0.9403\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 2s 250us/step - loss: 0.2998 - acc: 0.8962 - val_loss: 0.1582 - val_acc: 0.9444\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 2s 301us/step - loss: 0.2806 - acc: 0.9006 - val_loss: 0.1694 - val_acc: 0.9379\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 2s 306us/step - loss: 0.2534 - acc: 0.9120 - val_loss: 0.1657 - val_acc: 0.9423\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 2s 252us/step - loss: 0.2277 - acc: 0.9153 - val_loss: 0.1750 - val_acc: 0.9348\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.2299 - acc: 0.9181 - val_loss: 0.1698 - val_acc: 0.9396\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.2204 - acc: 0.9188 - val_loss: 0.1662 - val_acc: 0.9413\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.2164 - acc: 0.9238 - val_loss: 0.1970 - val_acc: 0.9294\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.2024 - acc: 0.9282 - val_loss: 0.1637 - val_acc: 0.9423\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.1913 - acc: 0.9334 - val_loss: 0.1524 - val_acc: 0.9464\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1809 - acc: 0.9351 - val_loss: 0.1681 - val_acc: 0.9362\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 2s 255us/step - loss: 0.1750 - acc: 0.9378 - val_loss: 0.1740 - val_acc: 0.9342\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1733 - acc: 0.9370 - val_loss: 0.1849 - val_acc: 0.9345\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1689 - acc: 0.9397 - val_loss: 0.1652 - val_acc: 0.9386\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.1718 - acc: 0.9362 - val_loss: 0.1952 - val_acc: 0.9325\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.1592 - acc: 0.9389 - val_loss: 0.1726 - val_acc: 0.9413\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.1557 - acc: 0.9431 - val_loss: 0.1737 - val_acc: 0.9406\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 2s 256us/step - loss: 0.1498 - acc: 0.9472 - val_loss: 0.1499 - val_acc: 0.9464\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 2s 257us/step - loss: 0.1548 - acc: 0.9419 - val_loss: 0.1784 - val_acc: 0.9332\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 2s 256us/step - loss: 0.1421 - acc: 0.9453 - val_loss: 0.1780 - val_acc: 0.9338\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 2s 256us/step - loss: 0.1517 - acc: 0.9431 - val_loss: 0.1599 - val_acc: 0.9444\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 2s 259us/step - loss: 0.1392 - acc: 0.9497 - val_loss: 0.1544 - val_acc: 0.9498\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 2s 259us/step - loss: 0.1387 - acc: 0.9487 - val_loss: 0.1827 - val_acc: 0.9393\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 2s 257us/step - loss: 0.1296 - acc: 0.9512 - val_loss: 0.1851 - val_acc: 0.9359\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.1357 - acc: 0.9471 - val_loss: 0.1714 - val_acc: 0.9423\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 2s 259us/step - loss: 0.1393 - acc: 0.9487 - val_loss: 0.1547 - val_acc: 0.9505\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.1309 - acc: 0.9521 - val_loss: 0.1526 - val_acc: 0.9488\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.1201 - acc: 0.9547 - val_loss: 0.1612 - val_acc: 0.9454\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.1246 - acc: 0.9551 - val_loss: 0.1703 - val_acc: 0.9450\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.1229 - acc: 0.9520 - val_loss: 0.1709 - val_acc: 0.9376\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 2s 261us/step - loss: 0.1277 - acc: 0.9542 - val_loss: 0.1640 - val_acc: 0.9481\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.1228 - acc: 0.9536 - val_loss: 0.1531 - val_acc: 0.9511\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 2s 264us/step - loss: 0.1231 - acc: 0.9544 - val_loss: 0.1637 - val_acc: 0.9430\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.1273 - acc: 0.9546 - val_loss: 0.1599 - val_acc: 0.9464\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.1141 - acc: 0.9572 - val_loss: 0.1751 - val_acc: 0.9410\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 2s 270us/step - loss: 0.1217 - acc: 0.9572 - val_loss: 0.1685 - val_acc: 0.9416\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 2s 268us/step - loss: 0.1119 - acc: 0.9608 - val_loss: 0.1918 - val_acc: 0.9389\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 2s 277us/step - loss: 0.1140 - acc: 0.9587 - val_loss: 0.1627 - val_acc: 0.9474\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 2s 276us/step - loss: 0.1235 - acc: 0.9557 - val_loss: 0.1602 - val_acc: 0.9481\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 2s 278us/step - loss: 0.1153 - acc: 0.9587 - val_loss: 0.1683 - val_acc: 0.9450\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 2s 272us/step - loss: 0.1121 - acc: 0.9618 - val_loss: 0.1511 - val_acc: 0.9494\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 2s 278us/step - loss: 0.1078 - acc: 0.9611 - val_loss: 0.1488 - val_acc: 0.9501\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 2s 275us/step - loss: 0.1069 - acc: 0.9614 - val_loss: 0.1631 - val_acc: 0.9481\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.0989 - acc: 0.9631 - val_loss: 0.1628 - val_acc: 0.9447\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1038 - acc: 0.9623 - val_loss: 0.1663 - val_acc: 0.9447\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.1043 - acc: 0.9597 - val_loss: 0.1552 - val_acc: 0.9494\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.1054 - acc: 0.9606 - val_loss: 0.1437 - val_acc: 0.9481\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 2s 268us/step - loss: 0.1044 - acc: 0.9612 - val_loss: 0.1739 - val_acc: 0.9444\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1061 - acc: 0.9616 - val_loss: 0.1715 - val_acc: 0.9416\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.1077 - acc: 0.9587 - val_loss: 0.1595 - val_acc: 0.9430\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 2s 273us/step - loss: 0.0993 - acc: 0.9629 - val_loss: 0.1669 - val_acc: 0.9444\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 2s 267us/step - loss: 0.0976 - acc: 0.9631 - val_loss: 0.1760 - val_acc: 0.9457\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.0973 - acc: 0.9646 - val_loss: 0.1931 - val_acc: 0.9460\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 2s 268us/step - loss: 0.1063 - acc: 0.9640 - val_loss: 0.1660 - val_acc: 0.9464\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.0973 - acc: 0.9637 - val_loss: 0.1802 - val_acc: 0.9471\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 2s 271us/step - loss: 0.0978 - acc: 0.9649 - val_loss: 0.1821 - val_acc: 0.9481\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 2s 276us/step - loss: 0.0941 - acc: 0.9652 - val_loss: 0.1763 - val_acc: 0.9444\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352/7352 [==============================] - 2s 247us/step - loss: 0.1047 - acc: 0.9630 - val_loss: 0.1594 - val_acc: 0.9515\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 2s 239us/step - loss: 0.1001 - acc: 0.9642 - val_loss: 0.1621 - val_acc: 0.9505\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.0978 - acc: 0.9646 - val_loss: 0.1720 - val_acc: 0.9494\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 2s 241us/step - loss: 0.0984 - acc: 0.9659 - val_loss: 0.1853 - val_acc: 0.9427\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.0927 - acc: 0.9687 - val_loss: 0.1636 - val_acc: 0.9481\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.0968 - acc: 0.9652 - val_loss: 0.1440 - val_acc: 0.9525\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.0973 - acc: 0.9637 - val_loss: 0.1532 - val_acc: 0.9515\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.0945 - acc: 0.9671 - val_loss: 0.1866 - val_acc: 0.9416\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 2s 245us/step - loss: 0.0973 - acc: 0.9638 - val_loss: 0.1633 - val_acc: 0.9481\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 2s 249us/step - loss: 0.0869 - acc: 0.9682 - val_loss: 0.1612 - val_acc: 0.9525\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.0901 - acc: 0.9672 - val_loss: 0.1515 - val_acc: 0.9518\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.0914 - acc: 0.9665 - val_loss: 0.1633 - val_acc: 0.9481\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 2s 245us/step - loss: 0.0867 - acc: 0.9672 - val_loss: 0.1735 - val_acc: 0.9484\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.0914 - acc: 0.9650 - val_loss: 0.1702 - val_acc: 0.9481\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.0869 - acc: 0.9680 - val_loss: 0.1382 - val_acc: 0.9555\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.0853 - acc: 0.9703 - val_loss: 0.1546 - val_acc: 0.9518\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 2s 250us/step - loss: 0.0881 - acc: 0.9683 - val_loss: 0.1503 - val_acc: 0.9539\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.0849 - acc: 0.9698 - val_loss: 0.1570 - val_acc: 0.9532\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.0935 - acc: 0.9661 - val_loss: 0.1787 - val_acc: 0.9450\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 2s 242us/step - loss: 0.0823 - acc: 0.9708 - val_loss: 0.1664 - val_acc: 0.9508\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 2s 246us/step - loss: 0.0766 - acc: 0.9713 - val_loss: 0.1674 - val_acc: 0.9505\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.0907 - acc: 0.9668 - val_loss: 0.1586 - val_acc: 0.9505\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 2s 245us/step - loss: 0.0908 - acc: 0.9698 - val_loss: 0.1527 - val_acc: 0.9522\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.0888 - acc: 0.9687 - val_loss: 0.1671 - val_acc: 0.9501\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 2s 249us/step - loss: 0.0805 - acc: 0.9706 - val_loss: 0.1592 - val_acc: 0.9508\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 2s 242us/step - loss: 0.0909 - acc: 0.9690 - val_loss: 0.1832 - val_acc: 0.9454\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.0857 - acc: 0.9701 - val_loss: 0.1692 - val_acc: 0.9522\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.0889 - acc: 0.9686 - val_loss: 0.1620 - val_acc: 0.9477\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 2s 242us/step - loss: 0.0823 - acc: 0.9727 - val_loss: 0.1546 - val_acc: 0.9498\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.0780 - acc: 0.9720 - val_loss: 0.1874 - val_acc: 0.9484\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 2s 248us/step - loss: 0.0867 - acc: 0.9671 - val_loss: 0.1841 - val_acc: 0.9467\n",
      "Epoch 91/100\n",
      "7352/7352 [==============================] - 2s 241us/step - loss: 0.0787 - acc: 0.9709 - val_loss: 0.1689 - val_acc: 0.9471\n",
      "Epoch 92/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.0809 - acc: 0.9716 - val_loss: 0.1682 - val_acc: 0.9494\n",
      "Epoch 93/100\n",
      "7352/7352 [==============================] - 2s 250us/step - loss: 0.0844 - acc: 0.9701 - val_loss: 0.1819 - val_acc: 0.9477\n",
      "Epoch 94/100\n",
      "7352/7352 [==============================] - 2s 247us/step - loss: 0.0811 - acc: 0.9703 - val_loss: 0.1802 - val_acc: 0.9467\n",
      "Epoch 95/100\n",
      "7352/7352 [==============================] - 2s 245us/step - loss: 0.0838 - acc: 0.9693 - val_loss: 0.2147 - val_acc: 0.9399\n",
      "Epoch 96/100\n",
      "7352/7352 [==============================] - 2s 246us/step - loss: 0.0783 - acc: 0.9721 - val_loss: 0.1865 - val_acc: 0.9481\n",
      "Epoch 97/100\n",
      "7352/7352 [==============================] - 2s 251us/step - loss: 0.0770 - acc: 0.9729 - val_loss: 0.2181 - val_acc: 0.9467\n",
      "Epoch 98/100\n",
      "7352/7352 [==============================] - 2s 253us/step - loss: 0.0785 - acc: 0.9727 - val_loss: 0.1920 - val_acc: 0.9477\n",
      "Epoch 99/100\n",
      "7352/7352 [==============================] - 2s 245us/step - loss: 0.0743 - acc: 0.9733 - val_loss: 0.2120 - val_acc: 0.9437\n",
      "Epoch 100/100\n",
      "7352/7352 [==============================] - 2s 250us/step - loss: 0.0855 - acc: 0.9705 - val_loss: 0.1561 - val_acc: 0.9535\n"
     ]
    }
   ],
   "source": [
    "X_train_pca_lstm = np.reshape(X_train_pca, (X_train_pca.shape[0], 1, X_train_pca.shape[1]))\n",
    "X_test_pca_lstm = np.reshape(X_test_pca, (X_test_pca.shape[0], 1, X_test_pca.shape[1]))\n",
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(X_train_pca_lstm, y_train, batch_size=32, epochs = 100, validation_data=(X_test_pca_lstm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 1s 458us/step\n"
     ]
    }
   ],
   "source": [
    "score = clf.evaluate(X_test_pca_lstm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15605845257130418"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
