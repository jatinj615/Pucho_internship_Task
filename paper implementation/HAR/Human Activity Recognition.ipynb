{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('X_train.txt', header=None)\n",
    "X_train = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
       "1  0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
       "2  0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
       "3  0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
       "4  0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
       "\n",
       "        7         8         9      ...          551       552       553  \\\n",
       "0 -0.983185 -0.923527 -0.934724    ...    -0.074323 -0.298676 -0.710304   \n",
       "1 -0.974914 -0.957686 -0.943068    ...     0.158075 -0.595051 -0.861499   \n",
       "2 -0.963668 -0.977469 -0.938692    ...     0.414503 -0.390748 -0.760104   \n",
       "3 -0.982750 -0.989302 -0.938692    ...     0.404573 -0.117290 -0.482845   \n",
       "4 -0.979672 -0.990441 -0.942469    ...     0.087753 -0.351471 -0.699205   \n",
       "\n",
       "        554       555       556       557       558       559       560  \n",
       "0 -0.112754  0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627  \n",
       "1  0.053477 -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317  \n",
       "2 -0.118559  0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118  \n",
       "3 -0.036788 -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663  \n",
       "4  0.123320  0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892  \n",
       "\n",
       "[5 rows x 561 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274488</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.109141</td>\n",
       "      <td>-0.605438</td>\n",
       "      <td>-0.510938</td>\n",
       "      <td>-0.604754</td>\n",
       "      <td>-0.630512</td>\n",
       "      <td>-0.526907</td>\n",
       "      <td>-0.606150</td>\n",
       "      <td>-0.468604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125293</td>\n",
       "      <td>-0.307009</td>\n",
       "      <td>-0.625294</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.489547</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>-0.056515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.448734</td>\n",
       "      <td>0.502645</td>\n",
       "      <td>0.418687</td>\n",
       "      <td>0.424073</td>\n",
       "      <td>0.485942</td>\n",
       "      <td>0.414122</td>\n",
       "      <td>0.544547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.321011</td>\n",
       "      <td>0.307584</td>\n",
       "      <td>0.336787</td>\n",
       "      <td>0.448306</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.477975</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.297480</td>\n",
       "      <td>0.279122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995357</td>\n",
       "      <td>-0.999765</td>\n",
       "      <td>-0.976580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262975</td>\n",
       "      <td>-0.024863</td>\n",
       "      <td>-0.120993</td>\n",
       "      <td>-0.992754</td>\n",
       "      <td>-0.978129</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>-0.993591</td>\n",
       "      <td>-0.978162</td>\n",
       "      <td>-0.980251</td>\n",
       "      <td>-0.936219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023692</td>\n",
       "      <td>-0.542602</td>\n",
       "      <td>-0.845573</td>\n",
       "      <td>-0.121527</td>\n",
       "      <td>-0.289549</td>\n",
       "      <td>-0.482273</td>\n",
       "      <td>-0.376341</td>\n",
       "      <td>-0.812065</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.143414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277193</td>\n",
       "      <td>-0.017219</td>\n",
       "      <td>-0.108676</td>\n",
       "      <td>-0.946196</td>\n",
       "      <td>-0.851897</td>\n",
       "      <td>-0.859365</td>\n",
       "      <td>-0.950709</td>\n",
       "      <td>-0.857328</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.881637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>-0.343685</td>\n",
       "      <td>-0.711692</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.709417</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288461</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.097794</td>\n",
       "      <td>-0.242813</td>\n",
       "      <td>-0.034231</td>\n",
       "      <td>-0.262415</td>\n",
       "      <td>-0.292680</td>\n",
       "      <td>-0.066701</td>\n",
       "      <td>-0.265671</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289096</td>\n",
       "      <td>-0.126979</td>\n",
       "      <td>-0.503878</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.292861</td>\n",
       "      <td>0.506187</td>\n",
       "      <td>0.359368</td>\n",
       "      <td>-0.509079</td>\n",
       "      <td>0.248353</td>\n",
       "      <td>0.107659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.274488    -0.017695    -0.109141    -0.605438    -0.510938   \n",
       "std       0.070261     0.040811     0.056635     0.448734     0.502645   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -0.999873   \n",
       "25%       0.262975    -0.024863    -0.120993    -0.992754    -0.978129   \n",
       "50%       0.277193    -0.017219    -0.108676    -0.946196    -0.851897   \n",
       "75%       0.288461    -0.010783    -0.097794    -0.242813    -0.034231   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.916238   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean     -0.604754    -0.630512    -0.526907    -0.606150    -0.468604   \n",
       "std       0.418687     0.424073     0.485942     0.414122     0.544547   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.980233    -0.993591    -0.978162    -0.980251    -0.936219   \n",
       "50%      -0.859365    -0.950709    -0.857328    -0.857143    -0.881637   \n",
       "75%      -0.262415    -0.292680    -0.066701    -0.265671    -0.017129   \n",
       "max       1.000000     1.000000     0.967664     1.000000     1.000000   \n",
       "\n",
       "          ...               551          552          553          554  \\\n",
       "count     ...       7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      ...          0.125293    -0.307009    -0.625294     0.008684   \n",
       "std       ...          0.250994     0.321011     0.307584     0.336787   \n",
       "min       ...         -1.000000    -0.995357    -0.999765    -0.976580   \n",
       "25%       ...         -0.023692    -0.542602    -0.845573    -0.121527   \n",
       "50%       ...          0.134000    -0.343685    -0.711692     0.009509   \n",
       "75%       ...          0.289096    -0.126979    -0.503878     0.150865   \n",
       "max       ...          0.946700     0.989538     0.956845     1.000000   \n",
       "\n",
       "               555          556          557          558          559  \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.002186     0.008726    -0.005981    -0.489547     0.058593   \n",
       "std       0.448306     0.608303     0.477975     0.511807     0.297480   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.289549    -0.482273    -0.376341    -0.812065    -0.017885   \n",
       "50%       0.008943     0.008735    -0.000368    -0.709417     0.182071   \n",
       "75%       0.292861     0.506187     0.359368    -0.509079     0.248353   \n",
       "max       1.000000     0.998702     0.996078     1.000000     0.478157   \n",
       "\n",
       "               560  \n",
       "count  7352.000000  \n",
       "mean     -0.056515  \n",
       "std       0.279122  \n",
       "min      -1.000000  \n",
       "25%      -0.143414  \n",
       "50%       0.003181  \n",
       "75%       0.107659  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  5\n",
       "1  5\n",
       "2  5\n",
       "3  5\n",
       "4  5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_fwf('y_train.txt',header=None)\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    0.191376\n",
       "5    0.186888\n",
       "4    0.174918\n",
       "1    0.166757\n",
       "2    0.145947\n",
       "3    0.134113\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0].value_counts()/target[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5  6\n",
       "0  0  0  0  0  1  0\n",
       "1  0  0  0  0  1  0\n",
       "2  0  0  0  0  1  0\n",
       "3  0  0  0  0  1  0\n",
       "4  0  0  0  0  1  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_fwf('X_test.txt',header=None)\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.read_fwf('y_test.txt', header=None)\n",
    "y_test = pd.get_dummies(test_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=60, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=60, units=50)`\n",
      "  \n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=50)`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=50)`\n",
      "  \n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"softmax\", units=6)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clf = Sequential()\n",
    "clf.add(Dense(output_dim = 50, kernel_initializer='uniform', activation='relu', input_dim = 60))\n",
    "clf.add(Dropout(rate=0.2))\n",
    "clf.add(Dense(output_dim = 50, kernel_initializer='uniform', activation='relu'))\n",
    "clf.add(Dropout(rate= 0.2))\n",
    "clf.add(Dense(output_dim = 50, kernel_initializer='uniform', activation='relu'))\n",
    "clf.add(Dropout(rate= 0.2))\n",
    "clf.add(Dense(output_dim = 6, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 3s 381us/step - loss: 1.1171 - acc: 0.4181 - val_loss: 0.5824 - val_acc: 0.7092\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.2995 - acc: 0.8818 - val_loss: 0.1871 - val_acc: 0.9264\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.1520 - acc: 0.9434 - val_loss: 0.1892 - val_acc: 0.9270\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.1275 - acc: 0.9523 - val_loss: 0.1884 - val_acc: 0.9338\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.1059 - acc: 0.9625 - val_loss: 0.1866 - val_acc: 0.9332\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.0939 - acc: 0.9665 - val_loss: 0.1855 - val_acc: 0.9379\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 1s 167us/step - loss: 0.0917 - acc: 0.9679 - val_loss: 0.1872 - val_acc: 0.9325\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 1s 172us/step - loss: 0.0820 - acc: 0.9720 - val_loss: 0.1870 - val_acc: 0.9372\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 1s 168us/step - loss: 0.0716 - acc: 0.9744 - val_loss: 0.2192 - val_acc: 0.9318\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 1s 166us/step - loss: 0.0671 - acc: 0.9748 - val_loss: 0.2261 - val_acc: 0.9325\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 1s 194us/step - loss: 0.0633 - acc: 0.9796 - val_loss: 0.2480 - val_acc: 0.9213\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 2s 206us/step - loss: 0.0615 - acc: 0.9777 - val_loss: 0.2008 - val_acc: 0.9379\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.0561 - acc: 0.9808 - val_loss: 0.2338 - val_acc: 0.9338\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 1s 132us/step - loss: 0.0517 - acc: 0.9823 - val_loss: 0.2474 - val_acc: 0.9321\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 1s 178us/step - loss: 0.0522 - acc: 0.9808 - val_loss: 0.2611 - val_acc: 0.9315\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 1s 191us/step - loss: 0.0484 - acc: 0.9831 - val_loss: 0.2581 - val_acc: 0.9253\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 1s 184us/step - loss: 0.0441 - acc: 0.9846 - val_loss: 0.2619 - val_acc: 0.9291\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.9837 - val_loss: 0.2747 - val_acc: 0.9226\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.0416 - acc: 0.9839 - val_loss: 0.2916 - val_acc: 0.9250\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 1s 185us/step - loss: 0.0332 - acc: 0.9884 - val_loss: 0.2453 - val_acc: 0.9372\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 1s 173us/step - loss: 0.0409 - acc: 0.9864 - val_loss: 0.2409 - val_acc: 0.9315\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 1s 174us/step - loss: 0.0360 - acc: 0.9876 - val_loss: 0.2375 - val_acc: 0.9348\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 1s 167us/step - loss: 0.0345 - acc: 0.9891 - val_loss: 0.2382 - val_acc: 0.9328\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.0329 - acc: 0.9884 - val_loss: 0.2637 - val_acc: 0.9335\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.0355 - acc: 0.9890 - val_loss: 0.2828 - val_acc: 0.9359\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.0357 - acc: 0.9880 - val_loss: 0.2781 - val_acc: 0.9325\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 1s 161us/step - loss: 0.0304 - acc: 0.9901 - val_loss: 0.3261 - val_acc: 0.9267\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 1s 174us/step - loss: 0.0293 - acc: 0.9903 - val_loss: 0.3343 - val_acc: 0.9247\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 1s 195us/step - loss: 0.0289 - acc: 0.9893 - val_loss: 0.2892 - val_acc: 0.9284\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 1s 172us/step - loss: 0.0287 - acc: 0.9891 - val_loss: 0.2663 - val_acc: 0.9338\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 1s 171us/step - loss: 0.0232 - acc: 0.9927 - val_loss: 0.2981 - val_acc: 0.9325\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 1s 173us/step - loss: 0.0277 - acc: 0.9906 - val_loss: 0.2924 - val_acc: 0.9304\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 1s 160us/step - loss: 0.0304 - acc: 0.9893 - val_loss: 0.2871 - val_acc: 0.9311\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 1s 167us/step - loss: 0.0237 - acc: 0.9921 - val_loss: 0.2731 - val_acc: 0.9369\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 1s 183us/step - loss: 0.0228 - acc: 0.9924 - val_loss: 0.3045 - val_acc: 0.9335\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 2s 223us/step - loss: 0.0231 - acc: 0.9918 - val_loss: 0.3115 - val_acc: 0.9308\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.0227 - acc: 0.9918 - val_loss: 0.3226 - val_acc: 0.9321\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 1s 201us/step - loss: 0.0292 - acc: 0.9898 - val_loss: 0.3392 - val_acc: 0.9264\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 1s 189us/step - loss: 0.0252 - acc: 0.9924 - val_loss: 0.2982 - val_acc: 0.9338\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 1s 175us/step - loss: 0.0201 - acc: 0.9943 - val_loss: 0.3243 - val_acc: 0.9304\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 1s 166us/step - loss: 0.0258 - acc: 0.9909 - val_loss: 0.2871 - val_acc: 0.9348\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 1s 176us/step - loss: 0.0206 - acc: 0.9929 - val_loss: 0.2985 - val_acc: 0.9332\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 2s 221us/step - loss: 0.0155 - acc: 0.9956 - val_loss: 0.2968 - val_acc: 0.9308\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.0233 - acc: 0.9931 - val_loss: 0.3072 - val_acc: 0.9304\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.0193 - acc: 0.9927 - val_loss: 0.3046 - val_acc: 0.9365\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 2s 218us/step - loss: 0.0248 - acc: 0.9920 - val_loss: 0.2785 - val_acc: 0.9389\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0164 - acc: 0.9948 - val_loss: 0.2941 - val_acc: 0.9362\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 1s 197us/step - loss: 0.0174 - acc: 0.9948 - val_loss: 0.2986 - val_acc: 0.9335\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 2s 219us/step - loss: 0.0215 - acc: 0.9932 - val_loss: 0.2878 - val_acc: 0.9318\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 1s 172us/step - loss: 0.0189 - acc: 0.9939 - val_loss: 0.3237 - val_acc: 0.9304\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 1s 171us/step - loss: 0.0181 - acc: 0.9951 - val_loss: 0.3742 - val_acc: 0.9270\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.0212 - acc: 0.9925 - val_loss: 0.3102 - val_acc: 0.9338\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 1s 172us/step - loss: 0.0183 - acc: 0.9936 - val_loss: 0.3131 - val_acc: 0.9365\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 1s 166us/step - loss: 0.0203 - acc: 0.9946 - val_loss: 0.3195 - val_acc: 0.9369\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 1s 165us/step - loss: 0.0175 - acc: 0.9943 - val_loss: 0.3249 - val_acc: 0.9315\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.0214 - acc: 0.9928 - val_loss: 0.3327 - val_acc: 0.9321\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 1s 158us/step - loss: 0.0160 - acc: 0.9951 - val_loss: 0.3476 - val_acc: 0.9321\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.0162 - acc: 0.9936 - val_loss: 0.3834 - val_acc: 0.9243\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.0148 - acc: 0.9950 - val_loss: 0.3955 - val_acc: 0.9311\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.0193 - acc: 0.9942 - val_loss: 0.3153 - val_acc: 0.9342\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.0164 - acc: 0.9937 - val_loss: 0.3176 - val_acc: 0.9311\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.0164 - acc: 0.9946 - val_loss: 0.3173 - val_acc: 0.9396\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.0233 - acc: 0.9932 - val_loss: 0.2611 - val_acc: 0.9406\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.0193 - acc: 0.9943 - val_loss: 0.2812 - val_acc: 0.9389\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.0170 - acc: 0.9947 - val_loss: 0.2855 - val_acc: 0.9389\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.0203 - acc: 0.9935 - val_loss: 0.2676 - val_acc: 0.9365\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.0154 - acc: 0.9947 - val_loss: 0.2927 - val_acc: 0.9338\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.0136 - acc: 0.9959 - val_loss: 0.3418 - val_acc: 0.9284\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.0235 - acc: 0.9920 - val_loss: 0.3339 - val_acc: 0.9304\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.0161 - acc: 0.9946 - val_loss: 0.3118 - val_acc: 0.9389\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.0190 - acc: 0.9939 - val_loss: 0.3082 - val_acc: 0.9277\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.0168 - acc: 0.9940 - val_loss: 0.3524 - val_acc: 0.9298\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0191 - acc: 0.9936 - val_loss: 0.3515 - val_acc: 0.9277\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.0189 - acc: 0.9944 - val_loss: 0.3474 - val_acc: 0.9321\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 1s 153us/step - loss: 0.0201 - acc: 0.9950 - val_loss: 0.4022 - val_acc: 0.9304\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 1s 160us/step - loss: 0.0139 - acc: 0.9955 - val_loss: 0.3580 - val_acc: 0.9318\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 1s 164us/step - loss: 0.0205 - acc: 0.9935 - val_loss: 0.3608 - val_acc: 0.9301\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 1s 142us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.3533 - val_acc: 0.9359\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.0175 - acc: 0.9946 - val_loss: 0.3686 - val_acc: 0.9284\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 1s 142us/step - loss: 0.0187 - acc: 0.9939 - val_loss: 0.3504 - val_acc: 0.9287\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 1s 160us/step - loss: 0.0138 - acc: 0.9951 - val_loss: 0.3503 - val_acc: 0.9321\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 1s 200us/step - loss: 0.0135 - acc: 0.9961 - val_loss: 0.3752 - val_acc: 0.9281\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 1s 168us/step - loss: 0.0134 - acc: 0.9959 - val_loss: 0.3796 - val_acc: 0.9311\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.0129 - acc: 0.9956 - val_loss: 0.3271 - val_acc: 0.9396\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 1s 171us/step - loss: 0.0170 - acc: 0.9951 - val_loss: 0.3376 - val_acc: 0.9396\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 2s 211us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.3324 - val_acc: 0.9389\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.0152 - acc: 0.9955 - val_loss: 0.3352 - val_acc: 0.9355\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.0134 - acc: 0.9962 - val_loss: 0.3272 - val_acc: 0.9382\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.3478 - val_acc: 0.9362\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 1s 194us/step - loss: 0.0119 - acc: 0.9951 - val_loss: 0.3528 - val_acc: 0.9403\n",
      "Epoch 91/100\n",
      "7352/7352 [==============================] - 1s 197us/step - loss: 0.0150 - acc: 0.9961 - val_loss: 0.3620 - val_acc: 0.9362\n",
      "Epoch 92/100\n",
      "7352/7352 [==============================] - 1s 180us/step - loss: 0.0184 - acc: 0.9940 - val_loss: 0.3076 - val_acc: 0.9413\n",
      "Epoch 93/100\n",
      "7352/7352 [==============================] - 1s 170us/step - loss: 0.0159 - acc: 0.9956 - val_loss: 0.3089 - val_acc: 0.9399\n",
      "Epoch 94/100\n",
      "7352/7352 [==============================] - 1s 142us/step - loss: 0.0107 - acc: 0.9969 - val_loss: 0.3579 - val_acc: 0.9382\n",
      "Epoch 95/100\n",
      "7352/7352 [==============================] - 1s 160us/step - loss: 0.0116 - acc: 0.9969 - val_loss: 0.3505 - val_acc: 0.9382\n",
      "Epoch 96/100\n",
      "7352/7352 [==============================] - 1s 190us/step - loss: 0.0164 - acc: 0.9952 - val_loss: 0.3564 - val_acc: 0.9338\n",
      "Epoch 97/100\n",
      "7352/7352 [==============================] - 1s 197us/step - loss: 0.0091 - acc: 0.9974 - val_loss: 0.3674 - val_acc: 0.9359\n",
      "Epoch 98/100\n",
      "7352/7352 [==============================] - 1s 171us/step - loss: 0.0153 - acc: 0.9952 - val_loss: 0.3428 - val_acc: 0.9342\n",
      "Epoch 99/100\n",
      "7352/7352 [==============================] - 1s 192us/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.3855 - val_acc: 0.9304\n",
      "Epoch 100/100\n",
      "7352/7352 [==============================] - 1s 174us/step - loss: 0.0144 - acc: 0.9969 - val_loss: 0.4000 - val_acc: 0.9318\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(X_train_pca, y_train, batch_size=32, epochs = 100, validation_data=(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Sequential()\n",
    "clf.add(LSTM(units=120, activation='relu', input_shape=(None, 60), dropout=0.4, recurrent_dropout=0.4))\n",
    "clf.add(Dense(35, activation='relu'))\n",
    "clf.add(Dropout(rate=0.5))\n",
    "clf.add(Dense(35, activation='relu'))\n",
    "clf.add(Dropout(rate=0.5))\n",
    "clf.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 3s 397us/step - loss: 1.3432 - acc: 0.4518 - val_loss: 0.5497 - val_acc: 0.8965\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.7071 - acc: 0.7179 - val_loss: 0.2840 - val_acc: 0.9063\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 2s 302us/step - loss: 0.5252 - acc: 0.7980 - val_loss: 0.2229 - val_acc: 0.9257\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.4555 - acc: 0.8308 - val_loss: 0.2061 - val_acc: 0.9342\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 2s 297us/step - loss: 0.3955 - acc: 0.8587 - val_loss: 0.1887 - val_acc: 0.9335\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 2s 276us/step - loss: 0.3622 - acc: 0.8678 - val_loss: 0.2152 - val_acc: 0.9213\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 2s 282us/step - loss: 0.3420 - acc: 0.8749 - val_loss: 0.1658 - val_acc: 0.9372\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.3413 - acc: 0.8834 - val_loss: 0.1643 - val_acc: 0.9423\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.3167 - acc: 0.8915 - val_loss: 0.2004 - val_acc: 0.9260\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.3001 - acc: 0.8950 - val_loss: 0.2175 - val_acc: 0.9240\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 2s 320us/step - loss: 0.2870 - acc: 0.9003 - val_loss: 0.1828 - val_acc: 0.9355\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 2s 273us/step - loss: 0.2808 - acc: 0.9063 - val_loss: 0.1929 - val_acc: 0.9281\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 2s 298us/step - loss: 0.2683 - acc: 0.9034 - val_loss: 0.1758 - val_acc: 0.9379\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.2555 - acc: 0.9135 - val_loss: 0.1811 - val_acc: 0.9338\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 2s 306us/step - loss: 0.2523 - acc: 0.9112 - val_loss: 0.1774 - val_acc: 0.9389\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 2s 288us/step - loss: 0.2443 - acc: 0.9132 - val_loss: 0.1818 - val_acc: 0.9332\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 2s 232us/step - loss: 0.2539 - acc: 0.9131 - val_loss: 0.1807 - val_acc: 0.9304\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 2s 289us/step - loss: 0.2450 - acc: 0.9138 - val_loss: 0.1835 - val_acc: 0.9386\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 2s 292us/step - loss: 0.2242 - acc: 0.9210 - val_loss: 0.2245 - val_acc: 0.9203\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 2s 249us/step - loss: 0.2404 - acc: 0.9170 - val_loss: 0.1974 - val_acc: 0.9332\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 2s 278us/step - loss: 0.2422 - acc: 0.9184 - val_loss: 0.1813 - val_acc: 0.9376\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 2s 278us/step - loss: 0.2191 - acc: 0.9261 - val_loss: 0.1644 - val_acc: 0.9440\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 2s 301us/step - loss: 0.2276 - acc: 0.9188 - val_loss: 0.1689 - val_acc: 0.9430\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.2080 - acc: 0.9266 - val_loss: 0.1698 - val_acc: 0.9416\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 2s 217us/step - loss: 0.2176 - acc: 0.9242 - val_loss: 0.1892 - val_acc: 0.9379\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 2s 221us/step - loss: 0.2133 - acc: 0.9251 - val_loss: 0.1649 - val_acc: 0.9464\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.2020 - acc: 0.9295 - val_loss: 0.1790 - val_acc: 0.9454\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 2s 281us/step - loss: 0.2116 - acc: 0.9286 - val_loss: 0.1954 - val_acc: 0.9342\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 2s 296us/step - loss: 0.1951 - acc: 0.9293 - val_loss: 0.2118 - val_acc: 0.9301\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 2s 236us/step - loss: 0.1963 - acc: 0.9320 - val_loss: 0.1949 - val_acc: 0.9382\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 2s 219us/step - loss: 0.2147 - acc: 0.9260 - val_loss: 0.1958 - val_acc: 0.9376\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 2s 234us/step - loss: 0.1917 - acc: 0.9336 - val_loss: 0.1691 - val_acc: 0.9450\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 2s 285us/step - loss: 0.2028 - acc: 0.9328 - val_loss: 0.1745 - val_acc: 0.9386\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 2s 289us/step - loss: 0.1861 - acc: 0.9361 - val_loss: 0.1868 - val_acc: 0.9406\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 2s 252us/step - loss: 0.1838 - acc: 0.9361 - val_loss: 0.1602 - val_acc: 0.9498\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1835 - acc: 0.9391 - val_loss: 0.1847 - val_acc: 0.9420\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 2s 227us/step - loss: 0.1794 - acc: 0.9384 - val_loss: 0.1948 - val_acc: 0.9406\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 2s 235us/step - loss: 0.1854 - acc: 0.9347 - val_loss: 0.1731 - val_acc: 0.9457\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 2s 233us/step - loss: 0.1729 - acc: 0.9389 - val_loss: 0.1637 - val_acc: 0.9522\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 2s 234us/step - loss: 0.1723 - acc: 0.9414 - val_loss: 0.1825 - val_acc: 0.9389\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 2s 236us/step - loss: 0.1657 - acc: 0.9464 - val_loss: 0.1767 - val_acc: 0.9484\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 2s 253us/step - loss: 0.1775 - acc: 0.9389 - val_loss: 0.2004 - val_acc: 0.9433\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 2s 233us/step - loss: 0.1639 - acc: 0.9434 - val_loss: 0.2127 - val_acc: 0.9396\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 2s 235us/step - loss: 0.1698 - acc: 0.9406 - val_loss: 0.1864 - val_acc: 0.9444\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.1710 - acc: 0.9400 - val_loss: 0.1697 - val_acc: 0.9454\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.1591 - acc: 0.9438 - val_loss: 0.1946 - val_acc: 0.9430\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 2s 242us/step - loss: 0.1655 - acc: 0.9395 - val_loss: 0.1892 - val_acc: 0.9413\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 2s 232us/step - loss: 0.1657 - acc: 0.9393 - val_loss: 0.1985 - val_acc: 0.9423\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 2s 253us/step - loss: 0.1602 - acc: 0.9465 - val_loss: 0.1941 - val_acc: 0.9454\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 2s 229us/step - loss: 0.1730 - acc: 0.9412 - val_loss: 0.1700 - val_acc: 0.9467\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 2s 233us/step - loss: 0.1524 - acc: 0.9449 - val_loss: 0.1745 - val_acc: 0.9457\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 2s 219us/step - loss: 0.1677 - acc: 0.9470 - val_loss: 0.1773 - val_acc: 0.9511\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 2s 238us/step - loss: 0.1664 - acc: 0.9407 - val_loss: 0.1828 - val_acc: 0.9437\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 2s 296us/step - loss: 0.1680 - acc: 0.9457 - val_loss: 0.1674 - val_acc: 0.9474\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 2s 284us/step - loss: 0.1572 - acc: 0.9474 - val_loss: 0.1806 - val_acc: 0.9437\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 2s 283us/step - loss: 0.1512 - acc: 0.9444 - val_loss: 0.1816 - val_acc: 0.9450\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 2s 272us/step - loss: 0.1552 - acc: 0.9475 - val_loss: 0.1733 - val_acc: 0.9508\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 2s 272us/step - loss: 0.1474 - acc: 0.9506 - val_loss: 0.1644 - val_acc: 0.9518\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 2s 270us/step - loss: 0.1570 - acc: 0.9474 - val_loss: 0.1923 - val_acc: 0.9467\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352/7352 [==============================] - 2s 251us/step - loss: 0.1549 - acc: 0.9442 - val_loss: 0.1933 - val_acc: 0.9450\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 2s 245us/step - loss: 0.1505 - acc: 0.9483 - val_loss: 0.1658 - val_acc: 0.9488\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 2s 251us/step - loss: 0.1583 - acc: 0.9467 - val_loss: 0.1715 - val_acc: 0.9457\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 2s 322us/step - loss: 0.1541 - acc: 0.9461 - val_loss: 0.1880 - val_acc: 0.9477\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 2s 271us/step - loss: 0.1522 - acc: 0.9518 - val_loss: 0.1987 - val_acc: 0.9396\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.1437 - acc: 0.9518 - val_loss: 0.1797 - val_acc: 0.9477\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 2s 252us/step - loss: 0.1435 - acc: 0.9513 - val_loss: 0.1703 - val_acc: 0.9511\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 2s 283us/step - loss: 0.1450 - acc: 0.9490 - val_loss: 0.1953 - val_acc: 0.9454\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 2s 270us/step - loss: 0.1373 - acc: 0.9535 - val_loss: 0.2003 - val_acc: 0.9450\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 2s 262us/step - loss: 0.1591 - acc: 0.9449 - val_loss: 0.1972 - val_acc: 0.9454\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 2s 255us/step - loss: 0.1401 - acc: 0.9516 - val_loss: 0.1802 - val_acc: 0.9450\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1527 - acc: 0.9508 - val_loss: 0.1781 - val_acc: 0.9525\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 2s 253us/step - loss: 0.1445 - acc: 0.9531 - val_loss: 0.1877 - val_acc: 0.9444\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 2s 252us/step - loss: 0.1482 - acc: 0.9509 - val_loss: 0.1863 - val_acc: 0.9433\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1420 - acc: 0.9535 - val_loss: 0.1794 - val_acc: 0.9498\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.1292 - acc: 0.9542 - val_loss: 0.1904 - val_acc: 0.9481\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 2s 255us/step - loss: 0.1402 - acc: 0.9551 - val_loss: 0.1977 - val_acc: 0.9382\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 2s 256us/step - loss: 0.1450 - acc: 0.9527 - val_loss: 0.2169 - val_acc: 0.9430\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.1387 - acc: 0.9542 - val_loss: 0.2056 - val_acc: 0.9457\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.1329 - acc: 0.9518 - val_loss: 0.2325 - val_acc: 0.9389\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 2s 308us/step - loss: 0.1398 - acc: 0.9538 - val_loss: 0.2147 - val_acc: 0.9450\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 2s 307us/step - loss: 0.1407 - acc: 0.9527 - val_loss: 0.2266 - val_acc: 0.9444\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 2s 278us/step - loss: 0.1435 - acc: 0.9504 - val_loss: 0.2042 - val_acc: 0.9423\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 2s 278us/step - loss: 0.1197 - acc: 0.9614 - val_loss: 0.2653 - val_acc: 0.9301\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 2s 279us/step - loss: 0.1303 - acc: 0.9550 - val_loss: 0.2403 - val_acc: 0.9379\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 2s 281us/step - loss: 0.1424 - acc: 0.9548 - val_loss: 0.2047 - val_acc: 0.9454\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 2s 289us/step - loss: 0.1357 - acc: 0.9542 - val_loss: 0.2040 - val_acc: 0.9444\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 2s 282us/step - loss: 0.1270 - acc: 0.9574 - val_loss: 0.2256 - val_acc: 0.9376\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 2s 283us/step - loss: 0.1369 - acc: 0.9570 - val_loss: 0.2337 - val_acc: 0.9399\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 2s 295us/step - loss: 0.1237 - acc: 0.9596 - val_loss: 0.2112 - val_acc: 0.9447\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 2s 310us/step - loss: 0.1335 - acc: 0.9567 - val_loss: 0.2213 - val_acc: 0.9365\n",
      "Epoch 91/100\n",
      "1568/7352 [=====>........................] - ETA: 1s - loss: 0.1118 - acc: 0.9630"
     ]
    }
   ],
   "source": [
    "X_train_pca_lstm = np.reshape(X_train_pca, (X_train_pca.shape[0], 1, X_train_pca.shape[1]))\n",
    "X_test_pca_lstm = np.reshape(X_test_pca, (X_test_pca.shape[0], 1, X_test_pca.shape[1]))\n",
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(X_train_pca_lstm, y_train, batch_size=32, epochs = 100, validation_data=(X_test_pca_lstm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
