{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('X_train.txt', header=None)\n",
    "X_train = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
       "1  0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
       "2  0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
       "3  0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
       "4  0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
       "\n",
       "        7         8         9      ...          551       552       553  \\\n",
       "0 -0.983185 -0.923527 -0.934724    ...    -0.074323 -0.298676 -0.710304   \n",
       "1 -0.974914 -0.957686 -0.943068    ...     0.158075 -0.595051 -0.861499   \n",
       "2 -0.963668 -0.977469 -0.938692    ...     0.414503 -0.390748 -0.760104   \n",
       "3 -0.982750 -0.989302 -0.938692    ...     0.404573 -0.117290 -0.482845   \n",
       "4 -0.979672 -0.990441 -0.942469    ...     0.087753 -0.351471 -0.699205   \n",
       "\n",
       "        554       555       556       557       558       559       560  \n",
       "0 -0.112754  0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627  \n",
       "1  0.053477 -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317  \n",
       "2 -0.118559  0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118  \n",
       "3 -0.036788 -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663  \n",
       "4  0.123320  0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892  \n",
       "\n",
       "[5 rows x 561 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274488</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.109141</td>\n",
       "      <td>-0.605438</td>\n",
       "      <td>-0.510938</td>\n",
       "      <td>-0.604754</td>\n",
       "      <td>-0.630512</td>\n",
       "      <td>-0.526907</td>\n",
       "      <td>-0.606150</td>\n",
       "      <td>-0.468604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125293</td>\n",
       "      <td>-0.307009</td>\n",
       "      <td>-0.625294</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.489547</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>-0.056515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.448734</td>\n",
       "      <td>0.502645</td>\n",
       "      <td>0.418687</td>\n",
       "      <td>0.424073</td>\n",
       "      <td>0.485942</td>\n",
       "      <td>0.414122</td>\n",
       "      <td>0.544547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.321011</td>\n",
       "      <td>0.307584</td>\n",
       "      <td>0.336787</td>\n",
       "      <td>0.448306</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.477975</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.297480</td>\n",
       "      <td>0.279122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995357</td>\n",
       "      <td>-0.999765</td>\n",
       "      <td>-0.976580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262975</td>\n",
       "      <td>-0.024863</td>\n",
       "      <td>-0.120993</td>\n",
       "      <td>-0.992754</td>\n",
       "      <td>-0.978129</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>-0.993591</td>\n",
       "      <td>-0.978162</td>\n",
       "      <td>-0.980251</td>\n",
       "      <td>-0.936219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023692</td>\n",
       "      <td>-0.542602</td>\n",
       "      <td>-0.845573</td>\n",
       "      <td>-0.121527</td>\n",
       "      <td>-0.289549</td>\n",
       "      <td>-0.482273</td>\n",
       "      <td>-0.376341</td>\n",
       "      <td>-0.812065</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.143414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277193</td>\n",
       "      <td>-0.017219</td>\n",
       "      <td>-0.108676</td>\n",
       "      <td>-0.946196</td>\n",
       "      <td>-0.851897</td>\n",
       "      <td>-0.859365</td>\n",
       "      <td>-0.950709</td>\n",
       "      <td>-0.857328</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.881637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>-0.343685</td>\n",
       "      <td>-0.711692</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.709417</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288461</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.097794</td>\n",
       "      <td>-0.242813</td>\n",
       "      <td>-0.034231</td>\n",
       "      <td>-0.262415</td>\n",
       "      <td>-0.292680</td>\n",
       "      <td>-0.066701</td>\n",
       "      <td>-0.265671</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289096</td>\n",
       "      <td>-0.126979</td>\n",
       "      <td>-0.503878</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.292861</td>\n",
       "      <td>0.506187</td>\n",
       "      <td>0.359368</td>\n",
       "      <td>-0.509079</td>\n",
       "      <td>0.248353</td>\n",
       "      <td>0.107659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.274488    -0.017695    -0.109141    -0.605438    -0.510938   \n",
       "std       0.070261     0.040811     0.056635     0.448734     0.502645   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -0.999873   \n",
       "25%       0.262975    -0.024863    -0.120993    -0.992754    -0.978129   \n",
       "50%       0.277193    -0.017219    -0.108676    -0.946196    -0.851897   \n",
       "75%       0.288461    -0.010783    -0.097794    -0.242813    -0.034231   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.916238   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean     -0.604754    -0.630512    -0.526907    -0.606150    -0.468604   \n",
       "std       0.418687     0.424073     0.485942     0.414122     0.544547   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.980233    -0.993591    -0.978162    -0.980251    -0.936219   \n",
       "50%      -0.859365    -0.950709    -0.857328    -0.857143    -0.881637   \n",
       "75%      -0.262415    -0.292680    -0.066701    -0.265671    -0.017129   \n",
       "max       1.000000     1.000000     0.967664     1.000000     1.000000   \n",
       "\n",
       "          ...               551          552          553          554  \\\n",
       "count     ...       7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      ...          0.125293    -0.307009    -0.625294     0.008684   \n",
       "std       ...          0.250994     0.321011     0.307584     0.336787   \n",
       "min       ...         -1.000000    -0.995357    -0.999765    -0.976580   \n",
       "25%       ...         -0.023692    -0.542602    -0.845573    -0.121527   \n",
       "50%       ...          0.134000    -0.343685    -0.711692     0.009509   \n",
       "75%       ...          0.289096    -0.126979    -0.503878     0.150865   \n",
       "max       ...          0.946700     0.989538     0.956845     1.000000   \n",
       "\n",
       "               555          556          557          558          559  \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.002186     0.008726    -0.005981    -0.489547     0.058593   \n",
       "std       0.448306     0.608303     0.477975     0.511807     0.297480   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.289549    -0.482273    -0.376341    -0.812065    -0.017885   \n",
       "50%       0.008943     0.008735    -0.000368    -0.709417     0.182071   \n",
       "75%       0.292861     0.506187     0.359368    -0.509079     0.248353   \n",
       "max       1.000000     0.998702     0.996078     1.000000     0.478157   \n",
       "\n",
       "               560  \n",
       "count  7352.000000  \n",
       "mean     -0.056515  \n",
       "std       0.279122  \n",
       "min      -1.000000  \n",
       "25%      -0.143414  \n",
       "50%       0.003181  \n",
       "75%       0.107659  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  5\n",
       "1  5\n",
       "2  5\n",
       "3  5\n",
       "4  5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_fwf('y_train.txt',header=None)\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    0.191376\n",
       "5    0.186888\n",
       "4    0.174918\n",
       "1    0.166757\n",
       "2    0.145947\n",
       "3    0.134113\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0].value_counts()/target[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5  6\n",
       "0  0  0  0  0  1  0\n",
       "1  0  0  0  0  1  0\n",
       "2  0  0  0  0  1  0\n",
       "3  0  0  0  0  1  0\n",
       "4  0  0  0  0  1  0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_fwf('X_test.txt',header=None)\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.read_fwf('y_test.txt', header=None)\n",
    "y_test = pd.get_dummies(test_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=60, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=60, units=20)`\n",
      "  \n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=20)`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jatin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"softmax\", units=6)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clf = Sequential()\n",
    "clf.add(Dense(output_dim = 20, kernel_initializer='uniform', activation='relu', input_dim = 60))\n",
    "clf.add(Dropout(rate=0.4))\n",
    "clf.add(Dense(output_dim = 20, kernel_initializer='uniform', activation='relu'))\n",
    "clf.add(Dropout(rate= 0.3))\n",
    "clf.add(Dense(output_dim = 6, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 1s 161us/step - loss: 1.3198 - acc: 0.3862 - val_loss: 1.0114 - val_acc: 0.6335\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 1s 135us/step - loss: 0.8695 - acc: 0.5850 - val_loss: 0.6128 - val_acc: 0.7401\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 1s 135us/step - loss: 0.5995 - acc: 0.7329 - val_loss: 0.4255 - val_acc: 0.8707\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.4365 - acc: 0.8254 - val_loss: 0.2890 - val_acc: 0.9179\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.3432 - acc: 0.8700 - val_loss: 0.2226 - val_acc: 0.9240\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 1s 136us/step - loss: 0.2948 - acc: 0.8917 - val_loss: 0.2104 - val_acc: 0.9216\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 1s 158us/step - loss: 0.2687 - acc: 0.9030 - val_loss: 0.1919 - val_acc: 0.9267\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 1s 191us/step - loss: 0.2421 - acc: 0.9117 - val_loss: 0.1873 - val_acc: 0.9253\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 1s 150us/step - loss: 0.2406 - acc: 0.9120 - val_loss: 0.1885 - val_acc: 0.9240\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.2185 - acc: 0.9204 - val_loss: 0.1909 - val_acc: 0.9250\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.2127 - acc: 0.9249 - val_loss: 0.1805 - val_acc: 0.9301\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 1s 142us/step - loss: 0.2061 - acc: 0.9270 - val_loss: 0.1962 - val_acc: 0.9237\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 1s 161us/step - loss: 0.2003 - acc: 0.9275 - val_loss: 0.2018 - val_acc: 0.9233\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 1s 166us/step - loss: 0.2009 - acc: 0.9279 - val_loss: 0.2026 - val_acc: 0.9220\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 1s 142us/step - loss: 0.1923 - acc: 0.9308 - val_loss: 0.2040 - val_acc: 0.9216\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 1s 146us/step - loss: 0.1979 - acc: 0.9267 - val_loss: 0.1975 - val_acc: 0.9270\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 1s 152us/step - loss: 0.2005 - acc: 0.9316 - val_loss: 0.2086 - val_acc: 0.9240\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1769 - acc: 0.9381 - val_loss: 0.2116 - val_acc: 0.9247\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 1s 157us/step - loss: 0.1843 - acc: 0.9336 - val_loss: 0.2044 - val_acc: 0.9274\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.1742 - acc: 0.9387 - val_loss: 0.2147 - val_acc: 0.9240\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1765 - acc: 0.9380 - val_loss: 0.2199 - val_acc: 0.9243\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.1805 - acc: 0.9353 - val_loss: 0.2400 - val_acc: 0.9203\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.1784 - acc: 0.9384 - val_loss: 0.2374 - val_acc: 0.9196\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.1715 - acc: 0.9388 - val_loss: 0.2311 - val_acc: 0.9226\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.1680 - acc: 0.9399 - val_loss: 0.2378 - val_acc: 0.9213\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 1s 140us/step - loss: 0.1642 - acc: 0.9393 - val_loss: 0.2281 - val_acc: 0.9274\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 1s 162us/step - loss: 0.1657 - acc: 0.9392 - val_loss: 0.2379 - val_acc: 0.9267\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.1663 - acc: 0.9411 - val_loss: 0.2387 - val_acc: 0.9270\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.1594 - acc: 0.9414 - val_loss: 0.2744 - val_acc: 0.9186\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 1s 168us/step - loss: 0.1608 - acc: 0.9436 - val_loss: 0.2559 - val_acc: 0.9264\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 1s 142us/step - loss: 0.1631 - acc: 0.9419 - val_loss: 0.2625 - val_acc: 0.9223\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.1574 - acc: 0.9434 - val_loss: 0.2677 - val_acc: 0.9257\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.1646 - acc: 0.9436 - val_loss: 0.2621 - val_acc: 0.9247\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.1548 - acc: 0.9446 - val_loss: 0.2783 - val_acc: 0.9230\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1589 - acc: 0.9452 - val_loss: 0.2635 - val_acc: 0.9274\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 1s 132us/step - loss: 0.1541 - acc: 0.9441 - val_loss: 0.2708 - val_acc: 0.9250\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 1s 135us/step - loss: 0.1547 - acc: 0.9440 - val_loss: 0.2787 - val_acc: 0.9226\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 1s 130us/step - loss: 0.1524 - acc: 0.9444 - val_loss: 0.2921 - val_acc: 0.9264\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 1s 145us/step - loss: 0.1577 - acc: 0.9457 - val_loss: 0.2930 - val_acc: 0.9216\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 1s 134us/step - loss: 0.1430 - acc: 0.9483 - val_loss: 0.3029 - val_acc: 0.9247\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.1582 - acc: 0.9444 - val_loss: 0.3095 - val_acc: 0.9226\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.1486 - acc: 0.9486 - val_loss: 0.3088 - val_acc: 0.9250\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.1409 - acc: 0.9495 - val_loss: 0.3135 - val_acc: 0.9257\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.1530 - acc: 0.9474 - val_loss: 0.3154 - val_acc: 0.9240\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 1s 188us/step - loss: 0.1563 - acc: 0.9463 - val_loss: 0.3298 - val_acc: 0.9247\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 1s 162us/step - loss: 0.1434 - acc: 0.9514 - val_loss: 0.3156 - val_acc: 0.9226\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.1559 - acc: 0.9464 - val_loss: 0.3135 - val_acc: 0.9226\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1546 - acc: 0.9422 - val_loss: 0.3189 - val_acc: 0.9213\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.1385 - acc: 0.9513 - val_loss: 0.3331 - val_acc: 0.9209\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1577 - acc: 0.9430 - val_loss: 0.3215 - val_acc: 0.9233\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.1513 - acc: 0.9463 - val_loss: 0.3297 - val_acc: 0.9233\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 1s 147us/step - loss: 0.1472 - acc: 0.9504 - val_loss: 0.3340 - val_acc: 0.9230\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 1s 146us/step - loss: 0.1529 - acc: 0.9464 - val_loss: 0.3307 - val_acc: 0.9226\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.1407 - acc: 0.9506 - val_loss: 0.3333 - val_acc: 0.9230\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.1397 - acc: 0.9498 - val_loss: 0.3485 - val_acc: 0.9209\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.1469 - acc: 0.9509 - val_loss: 0.3423 - val_acc: 0.9253\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 1s 148us/step - loss: 0.1453 - acc: 0.9476 - val_loss: 0.3509 - val_acc: 0.9209\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.1388 - acc: 0.9512 - val_loss: 0.3508 - val_acc: 0.9220\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 1s 178us/step - loss: 0.1461 - acc: 0.9489 - val_loss: 0.3448 - val_acc: 0.9216\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352/7352 [==============================] - 1s 178us/step - loss: 0.1440 - acc: 0.9517 - val_loss: 0.3289 - val_acc: 0.9267\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 1s 141us/step - loss: 0.1402 - acc: 0.9550 - val_loss: 0.3407 - val_acc: 0.9260\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 1s 114us/step - loss: 0.1505 - acc: 0.9482 - val_loss: 0.3253 - val_acc: 0.9257\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 1s 139us/step - loss: 0.1429 - acc: 0.9470 - val_loss: 0.3396 - val_acc: 0.9237\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 1s 130us/step - loss: 0.1397 - acc: 0.9533 - val_loss: 0.3391 - val_acc: 0.9247\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 1s 120us/step - loss: 0.1406 - acc: 0.9490 - val_loss: 0.3331 - val_acc: 0.9270\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 1s 146us/step - loss: 0.1433 - acc: 0.9513 - val_loss: 0.3410 - val_acc: 0.9247\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 1s 156us/step - loss: 0.1373 - acc: 0.9513 - val_loss: 0.3372 - val_acc: 0.9277\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.1449 - acc: 0.9525 - val_loss: 0.3441 - val_acc: 0.9237\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 1s 138us/step - loss: 0.1452 - acc: 0.9475 - val_loss: 0.3445 - val_acc: 0.9226\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 1s 118us/step - loss: 0.1370 - acc: 0.9520 - val_loss: 0.3490 - val_acc: 0.9257\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 1s 201us/step - loss: 0.1435 - acc: 0.9532 - val_loss: 0.3587 - val_acc: 0.9240\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 1s 130us/step - loss: 0.1414 - acc: 0.9532 - val_loss: 0.3618 - val_acc: 0.9253\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 1s 177us/step - loss: 0.1460 - acc: 0.9512 - val_loss: 0.3255 - val_acc: 0.9270\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 1s 171us/step - loss: 0.1413 - acc: 0.9495 - val_loss: 0.3296 - val_acc: 0.9260\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 1s 167us/step - loss: 0.1343 - acc: 0.9551 - val_loss: 0.3492 - val_acc: 0.9253\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 1s 131us/step - loss: 0.1446 - acc: 0.9476 - val_loss: 0.3664 - val_acc: 0.9253\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 1s 113us/step - loss: 0.1379 - acc: 0.9528 - val_loss: 0.3466 - val_acc: 0.9260\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 1s 137us/step - loss: 0.1301 - acc: 0.9546 - val_loss: 0.3536 - val_acc: 0.9257\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 1s 143us/step - loss: 0.1404 - acc: 0.9494 - val_loss: 0.3542 - val_acc: 0.9237\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 1s 173us/step - loss: 0.1331 - acc: 0.9574 - val_loss: 0.3496 - val_acc: 0.9270\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 1s 144us/step - loss: 0.1429 - acc: 0.9502 - val_loss: 0.3522 - val_acc: 0.9257\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 1s 149us/step - loss: 0.1497 - acc: 0.9470 - val_loss: 0.3561 - val_acc: 0.9250\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.1458 - acc: 0.9486 - val_loss: 0.3356 - val_acc: 0.9264\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.1444 - acc: 0.9516 - val_loss: 0.3388 - val_acc: 0.9260\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.1345 - acc: 0.9514 - val_loss: 0.3621 - val_acc: 0.9196\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.1347 - acc: 0.9498 - val_loss: 0.3478 - val_acc: 0.9243\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 1s 157us/step - loss: 0.1292 - acc: 0.9491 - val_loss: 0.3649 - val_acc: 0.9206\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 1s 159us/step - loss: 0.1398 - acc: 0.9525 - val_loss: 0.3676 - val_acc: 0.9240\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 1s 151us/step - loss: 0.1327 - acc: 0.9525 - val_loss: 0.3660 - val_acc: 0.9250\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.1418 - acc: 0.9516 - val_loss: 0.3664 - val_acc: 0.9284\n",
      "Epoch 91/100\n",
      "7352/7352 [==============================] - 1s 186us/step - loss: 0.1406 - acc: 0.9491 - val_loss: 0.3873 - val_acc: 0.9203\n",
      "Epoch 92/100\n",
      "7352/7352 [==============================] - 1s 181us/step - loss: 0.1388 - acc: 0.9532 - val_loss: 0.3867 - val_acc: 0.9247\n",
      "Epoch 93/100\n",
      "7352/7352 [==============================] - 1s 179us/step - loss: 0.1331 - acc: 0.9513 - val_loss: 0.3796 - val_acc: 0.9260\n",
      "Epoch 94/100\n",
      "7352/7352 [==============================] - 1s 184us/step - loss: 0.1469 - acc: 0.9497 - val_loss: 0.3584 - val_acc: 0.9270\n",
      "Epoch 95/100\n",
      "7352/7352 [==============================] - 1s 174us/step - loss: 0.1422 - acc: 0.9516 - val_loss: 0.3797 - val_acc: 0.9250\n",
      "Epoch 96/100\n",
      "7352/7352 [==============================] - 1s 179us/step - loss: 0.1443 - acc: 0.9495 - val_loss: 0.3719 - val_acc: 0.9247\n",
      "Epoch 97/100\n",
      "7352/7352 [==============================] - 1s 180us/step - loss: 0.1386 - acc: 0.9498 - val_loss: 0.3664 - val_acc: 0.9237\n",
      "Epoch 98/100\n",
      "7352/7352 [==============================] - 1s 157us/step - loss: 0.1332 - acc: 0.9527 - val_loss: 0.3732 - val_acc: 0.9257\n",
      "Epoch 99/100\n",
      "7352/7352 [==============================] - 1s 154us/step - loss: 0.1377 - acc: 0.9514 - val_loss: 0.3574 - val_acc: 0.9264\n",
      "Epoch 100/100\n",
      "7352/7352 [==============================] - 1s 155us/step - loss: 0.1354 - acc: 0.9510 - val_loss: 0.3705 - val_acc: 0.9247\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(X_train_pca, y_train, batch_size=32, epochs = 100, validation_data=(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_clf = Sequential()\n",
    "lstm_clf.add(LSTM(units=40, activation='relu', input_shape=(None, 60), dropout=0.3, recurrent_dropout=0.3))\n",
    "lstm_clf.add(Dense(20, activation='relu'))\n",
    "lstm_clf.add(Dropout(rate=0.3))\n",
    "lstm_clf.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_clf.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 3s 416us/step - loss: 1.1482 - acc: 0.5992 - val_loss: 0.4665 - val_acc: 0.8962\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 2s 242us/step - loss: 0.4992 - acc: 0.8390 - val_loss: 0.2325 - val_acc: 0.9131\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.3438 - acc: 0.8821 - val_loss: 0.1879 - val_acc: 0.9277\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 2s 237us/step - loss: 0.2904 - acc: 0.8953 - val_loss: 0.1676 - val_acc: 0.9393\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 2s 238us/step - loss: 0.2610 - acc: 0.9040 - val_loss: 0.1728 - val_acc: 0.9342\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 2s 242us/step - loss: 0.2412 - acc: 0.9154 - val_loss: 0.1753 - val_acc: 0.9355\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.2169 - acc: 0.9236 - val_loss: 0.1738 - val_acc: 0.9389\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.2206 - acc: 0.9227 - val_loss: 0.1710 - val_acc: 0.9376\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 2s 239us/step - loss: 0.2049 - acc: 0.9298 - val_loss: 0.1760 - val_acc: 0.9345\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 2s 239us/step - loss: 0.2084 - acc: 0.9222 - val_loss: 0.1793 - val_acc: 0.9342\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 2s 247us/step - loss: 0.1946 - acc: 0.9295 - val_loss: 0.1759 - val_acc: 0.9369\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 2s 244us/step - loss: 0.1951 - acc: 0.9290 - val_loss: 0.1846 - val_acc: 0.9355\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 2s 240us/step - loss: 0.1931 - acc: 0.9286 - val_loss: 0.1647 - val_acc: 0.9403\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 2s 247us/step - loss: 0.1869 - acc: 0.9344 - val_loss: 0.1694 - val_acc: 0.9396\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 2s 256us/step - loss: 0.1840 - acc: 0.9334 - val_loss: 0.1777 - val_acc: 0.9321\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 2s 246us/step - loss: 0.1774 - acc: 0.9350 - val_loss: 0.1932 - val_acc: 0.9270\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 2s 284us/step - loss: 0.1797 - acc: 0.9317 - val_loss: 0.1785 - val_acc: 0.9386\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1735 - acc: 0.9331 - val_loss: 0.1829 - val_acc: 0.9362\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 2s 249us/step - loss: 0.1660 - acc: 0.9373 - val_loss: 0.1794 - val_acc: 0.9389\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 2s 248us/step - loss: 0.1807 - acc: 0.9368 - val_loss: 0.1872 - val_acc: 0.9321\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 2s 264us/step - loss: 0.1660 - acc: 0.9414 - val_loss: 0.2035 - val_acc: 0.9325\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1575 - acc: 0.9437 - val_loss: 0.1935 - val_acc: 0.9335\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 2s 250us/step - loss: 0.1635 - acc: 0.9411 - val_loss: 0.1900 - val_acc: 0.9332\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 2s 257us/step - loss: 0.1596 - acc: 0.9408 - val_loss: 0.1897 - val_acc: 0.9389\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 2s 251us/step - loss: 0.1560 - acc: 0.9430 - val_loss: 0.1967 - val_acc: 0.9348\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1534 - acc: 0.9468 - val_loss: 0.1924 - val_acc: 0.9352\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 2s 255us/step - loss: 0.1497 - acc: 0.9437 - val_loss: 0.1871 - val_acc: 0.9416\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.1440 - acc: 0.9486 - val_loss: 0.1895 - val_acc: 0.9396\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 2s 273us/step - loss: 0.1527 - acc: 0.9455 - val_loss: 0.1900 - val_acc: 0.9389\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 2s 257us/step - loss: 0.1440 - acc: 0.9465 - val_loss: 0.2040 - val_acc: 0.9362\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1417 - acc: 0.9475 - val_loss: 0.1842 - val_acc: 0.9393\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1447 - acc: 0.9475 - val_loss: 0.1913 - val_acc: 0.9376\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.1350 - acc: 0.9486 - val_loss: 0.1959 - val_acc: 0.9372\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.1411 - acc: 0.9476 - val_loss: 0.1968 - val_acc: 0.9386\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 2s 256us/step - loss: 0.1401 - acc: 0.9476 - val_loss: 0.2008 - val_acc: 0.9389\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 2s 267us/step - loss: 0.1422 - acc: 0.9512 - val_loss: 0.2017 - val_acc: 0.9379\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 2s 283us/step - loss: 0.1293 - acc: 0.9520 - val_loss: 0.2192 - val_acc: 0.9328\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 2s 271us/step - loss: 0.1370 - acc: 0.9478 - val_loss: 0.2284 - val_acc: 0.9315\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 2s 294us/step - loss: 0.1417 - acc: 0.9506 - val_loss: 0.2086 - val_acc: 0.9342\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 2s 253us/step - loss: 0.1306 - acc: 0.9532 - val_loss: 0.2156 - val_acc: 0.9362\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 2s 252us/step - loss: 0.1327 - acc: 0.9495 - val_loss: 0.2204 - val_acc: 0.9348\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1296 - acc: 0.9543 - val_loss: 0.2199 - val_acc: 0.9342\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 2s 252us/step - loss: 0.1366 - acc: 0.9516 - val_loss: 0.2090 - val_acc: 0.9365\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 2s 259us/step - loss: 0.1258 - acc: 0.9529 - val_loss: 0.2103 - val_acc: 0.9372\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 2s 275us/step - loss: 0.1206 - acc: 0.9544 - val_loss: 0.2163 - val_acc: 0.9359\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 2s 290us/step - loss: 0.1246 - acc: 0.9543 - val_loss: 0.2341 - val_acc: 0.9338\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 2s 250us/step - loss: 0.1210 - acc: 0.9576 - val_loss: 0.2485 - val_acc: 0.9311\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 2s 236us/step - loss: 0.1223 - acc: 0.9542 - val_loss: 0.2309 - val_acc: 0.9311\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 1s 200us/step - loss: 0.1121 - acc: 0.9593 - val_loss: 0.2137 - val_acc: 0.9403\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 1s 201us/step - loss: 0.1199 - acc: 0.9550 - val_loss: 0.2230 - val_acc: 0.9376\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 1s 196us/step - loss: 0.1225 - acc: 0.9581 - val_loss: 0.2267 - val_acc: 0.9382\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 1s 197us/step - loss: 0.1127 - acc: 0.9591 - val_loss: 0.2399 - val_acc: 0.9355\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 2s 251us/step - loss: 0.1188 - acc: 0.9581 - val_loss: 0.2219 - val_acc: 0.9359\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 2s 243us/step - loss: 0.1175 - acc: 0.9588 - val_loss: 0.2224 - val_acc: 0.9399\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 2s 223us/step - loss: 0.1173 - acc: 0.9574 - val_loss: 0.2234 - val_acc: 0.9372\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 2s 223us/step - loss: 0.1176 - acc: 0.9574 - val_loss: 0.2403 - val_acc: 0.9355\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 1s 199us/step - loss: 0.1188 - acc: 0.9577 - val_loss: 0.2192 - val_acc: 0.9376\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 1s 198us/step - loss: 0.1136 - acc: 0.9603 - val_loss: 0.2418 - val_acc: 0.9365\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 2s 205us/step - loss: 0.1140 - acc: 0.9589 - val_loss: 0.2411 - val_acc: 0.9338\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352/7352 [==============================] - 2s 233us/step - loss: 0.1106 - acc: 0.9608 - val_loss: 0.2537 - val_acc: 0.9318\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 2s 332us/step - loss: 0.1055 - acc: 0.9627 - val_loss: 0.2691 - val_acc: 0.9321\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 2s 297us/step - loss: 0.1191 - acc: 0.9577 - val_loss: 0.2653 - val_acc: 0.9332\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 2s 255us/step - loss: 0.1151 - acc: 0.9621 - val_loss: 0.2539 - val_acc: 0.9315\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 2s 256us/step - loss: 0.1100 - acc: 0.9611 - val_loss: 0.2750 - val_acc: 0.9332\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 2s 265us/step - loss: 0.1176 - acc: 0.9584 - val_loss: 0.2687 - val_acc: 0.9332\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 2s 277us/step - loss: 0.1039 - acc: 0.9649 - val_loss: 0.2365 - val_acc: 0.9376\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 2s 298us/step - loss: 0.1170 - acc: 0.9589 - val_loss: 0.2548 - val_acc: 0.9352\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 2s 259us/step - loss: 0.1047 - acc: 0.9600 - val_loss: 0.2511 - val_acc: 0.9342\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 2s 268us/step - loss: 0.1011 - acc: 0.9629 - val_loss: 0.2507 - val_acc: 0.9359\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 2s 222us/step - loss: 0.1007 - acc: 0.9642 - val_loss: 0.2538 - val_acc: 0.9379\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 2s 247us/step - loss: 0.1067 - acc: 0.9633 - val_loss: 0.2627 - val_acc: 0.9403\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 2s 226us/step - loss: 0.1042 - acc: 0.9633 - val_loss: 0.2561 - val_acc: 0.9399\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 2s 219us/step - loss: 0.1057 - acc: 0.9640 - val_loss: 0.2595 - val_acc: 0.9369\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 2s 227us/step - loss: 0.1116 - acc: 0.9604 - val_loss: 0.2599 - val_acc: 0.9376\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 2s 250us/step - loss: 0.1059 - acc: 0.9642 - val_loss: 0.2599 - val_acc: 0.9372\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 2s 217us/step - loss: 0.1105 - acc: 0.9622 - val_loss: 0.2420 - val_acc: 0.9382\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 2s 272us/step - loss: 0.1016 - acc: 0.9603 - val_loss: 0.2479 - val_acc: 0.9369\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 2s 254us/step - loss: 0.1010 - acc: 0.9650 - val_loss: 0.2604 - val_acc: 0.9389\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 2s 252us/step - loss: 0.1038 - acc: 0.9616 - val_loss: 0.2879 - val_acc: 0.9365\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 2s 246us/step - loss: 0.1017 - acc: 0.9637 - val_loss: 0.3036 - val_acc: 0.9291\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 2s 296us/step - loss: 0.1124 - acc: 0.9652 - val_loss: 0.2751 - val_acc: 0.9294\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 2s 212us/step - loss: 0.1034 - acc: 0.9661 - val_loss: 0.2806 - val_acc: 0.9321\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 2s 270us/step - loss: 0.0950 - acc: 0.9659 - val_loss: 0.2917 - val_acc: 0.9332\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 2s 280us/step - loss: 0.1029 - acc: 0.9640 - val_loss: 0.3088 - val_acc: 0.9318\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 2s 277us/step - loss: 0.1019 - acc: 0.9634 - val_loss: 0.2904 - val_acc: 0.9335\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 2s 267us/step - loss: 0.1014 - acc: 0.9657 - val_loss: 0.2722 - val_acc: 0.9389\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 2s 260us/step - loss: 0.0867 - acc: 0.9691 - val_loss: 0.2950 - val_acc: 0.9325\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 2s 269us/step - loss: 0.0970 - acc: 0.9682 - val_loss: 0.3081 - val_acc: 0.9328\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 2s 263us/step - loss: 0.1012 - acc: 0.9642 - val_loss: 0.3180 - val_acc: 0.9277\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 2s 268us/step - loss: 0.0968 - acc: 0.9693 - val_loss: 0.3322 - val_acc: 0.9304\n",
      "Epoch 91/100\n",
      "7352/7352 [==============================] - 2s 279us/step - loss: 0.0913 - acc: 0.9656 - val_loss: 0.3254 - val_acc: 0.9311\n",
      "Epoch 92/100\n",
      "7352/7352 [==============================] - 2s 258us/step - loss: 0.1070 - acc: 0.9650 - val_loss: 0.2872 - val_acc: 0.9372\n",
      "Epoch 93/100\n",
      "7352/7352 [==============================] - 2s 266us/step - loss: 0.0894 - acc: 0.9675 - val_loss: 0.3023 - val_acc: 0.9376\n",
      "Epoch 94/100\n",
      "7352/7352 [==============================] - 2s 271us/step - loss: 0.1009 - acc: 0.9649 - val_loss: 0.2946 - val_acc: 0.9348\n",
      "Epoch 95/100\n",
      "7352/7352 [==============================] - 2s 273us/step - loss: 0.1075 - acc: 0.9649 - val_loss: 0.2967 - val_acc: 0.9335\n",
      "Epoch 96/100\n",
      "7352/7352 [==============================] - 2s 275us/step - loss: 0.1021 - acc: 0.9652 - val_loss: 0.2820 - val_acc: 0.9352\n",
      "Epoch 97/100\n",
      "7352/7352 [==============================] - 2s 285us/step - loss: 0.0867 - acc: 0.9706 - val_loss: 0.2920 - val_acc: 0.9318\n",
      "Epoch 98/100\n",
      "7352/7352 [==============================] - 2s 267us/step - loss: 0.0986 - acc: 0.9675 - val_loss: 0.2918 - val_acc: 0.9348\n",
      "Epoch 99/100\n",
      "7352/7352 [==============================] - 2s 276us/step - loss: 0.0926 - acc: 0.9680 - val_loss: 0.2640 - val_acc: 0.9369\n",
      "Epoch 100/100\n",
      "7352/7352 [==============================] - 2s 264us/step - loss: 0.0978 - acc: 0.9679 - val_loss: 0.2804 - val_acc: 0.9359\n"
     ]
    }
   ],
   "source": [
    "X_train_pca_lstm = np.reshape(X_train_pca, (X_train_pca.shape[0], 1, X_train_pca.shape[1]))\n",
    "X_test_pca_lstm = np.reshape(X_test_pca, (X_test_pca.shape[0], 1, X_test_pca.shape[1]))\n",
    "with tf.device('/gpu:0'):\n",
    "    lstm_clf.fit(X_train_pca_lstm, y_train, batch_size=32, epochs = 100, validation_data=(X_test_pca_lstm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Ann model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model_json = clf.to_json()\n",
    "with open(\"ann_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "clf.save_weights(\"ann_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = lstm_clf.to_json()\n",
    "with open(\"lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "lstm_clf.save_weights(\"lstm_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
